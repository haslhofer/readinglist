## From scratch implementation of Self-Attention, Multi-Head Attention, Cross-Attention, and Causal Self-Attention in Large Language Models (LLMs)
Summary: This article explains the inner workings of the self-attention mechanism, a core component of large language models (LLMs) like GPT-4 and Llama, through a step-by-step coding approach. It also covers multi-head attention, cross-attention, and causal self-attention.

Link: https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention

<img src="/img/fc744586-928d-47fd-bd8e-87045144ad19.png" width="400" />


<sup><sub>1/15/2024 [Mark as read](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9895_0&tag=isread) [Mark as BestOf](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9895_0&tag=bestof) [Experiments](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9895_0&tag=Experiments)<sub/><sup/>

<br/><br/>

## This article lists recommended books of various genres, all of which promise to expand readers' minds.
Summary: This repository contains a list of mind-expanding books curated by various contributors. The selection covers a wide range of topics, including startups and business, philosophy and psychology, autobiographies and biographies, history, science and medicine, logic and problem-solving, politics, economics, gender, sexuality, race, education, writing, theater and film, Shakespeare, fiction, and miscellaneous subjects like health, design, travel, language, nature, and art. Some popular book recommendations are Shoe Dog by Phil Knight, The Ride of a Lifetime by Robert Iger, and Bad Blood by John Carreyrou.

Link: https://github.com/hackerkid/Mind-Expanding-Books

<img src="/img/9e8c5f7b-a2eb-403f-8998-875c15b5938d.png" width="400" />


<sup><sub>1/15/2024 [Mark as read](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9893_0&tag=isread) [Mark as BestOf](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9893_0&tag=bestof) [Experiments](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9893_0&tag=Experiments)<sub/><sup/>

<br/><br/>

## Automatic Evaluation Framework for Assessing LLMs' Protocol Planning Abilities in Biology
Summary: The paper presents a novel automatic evaluation framework and dataset called BioProt for assessing the performance of Large Language Models (LLMs) in planning experimental protocols in biology. The framework involves converting natural language protocols into pseudocode representations, which enables the evaluation of an LLM's ability to reconstruct the pseudocode from high-level descriptions and admissible pseudocode functions. The study explores the performance of GPT-3 and GPT-4 on this task and examines their robustness. It also demonstrates the utility of pseudocode representations by generating accurate novel protocols and successfully executing a generated protocol in a biological laboratory. The extensibility of the framework to other areas of science or domains lacking automatic evaluation is highlighted.

Link: https://arxiv.org/abs/2310.10632?utm_source=substack&utm_medium=email

<img src="/img/38d1a3b5-34b5-4974-bf30-dc849cad863c.png" width="400" />


<sup><sub>1/15/2024 [Mark as read](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9891_0&tag=isread) [Mark as BestOf](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9891_0&tag=bestof) [Experiments](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9891_0&tag=Experiments)<sub/><sup/>

<br/><br/>

## Explore topics, data connectivity and run network analysis with the Semantic Graph
Summary: A semantic graph, also known as a knowledge graph or semantic network, is introduced, constructed with semantic relationships connecting the nodes. Nodes and relationships can be added to the graph, and analysis functions can be run on it. Semantic graphs can be used to explore relationships, such as topics and interconnections in a dataset. Embeddings instances can be indexed into a graph, allowing for network analysis and topic modeling. Topic modeling can be done using community detection algorithms, and centrality and pagerank can be used to analyze the graph. The graph can also be traversed to show how nodes are connected. Furthermore, images can be grouped into topics using topic modeling, and the image graph can be walked to explore relationships between images.

Link: https://neuml.hashnode.dev/introducing-the-semantic-graph

<img src="/img/b9351921-70a5-4365-aec2-52c9c1ba5b74.png" width="400" />


<sup><sub>1/15/2024 [Mark as read](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9889_0&tag=isread) [Mark as BestOf](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9889_0&tag=bestof) [Experiments](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9889_0&tag=Experiments)<sub/><sup/>

<br/><br/>

## Open-source, Highly Accurate Optical Character Recognition (OCR) System Released
Summary: A new open-source Optical Character Recognition (OCR) tool called Surya has been released, which accurately extracts text from images and supports multiple languages. It can recognize text at the line level, making it a valuable tool for tasks such as document processing and data extraction.

Link: https://www.linkedin.com/posts/alexcarliera_a-new-highly-accurate-ocr-was-just-released-activity-7151966210732040192-MeDq?utm_source=share&amp;utm_medium=member_android

<img src="/img/5a96af17-2a6c-45f1-8a69-663aa357620c.png" width="400" />


<sup><sub>1/14/2024 [Mark as read](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9884_0&tag=isread) [Mark as BestOf](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9884_0&tag=bestof) [Experiments](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9884_0&tag=Experiments)<sub/><sup/>

<br/><br/>

## Sure, here is a one-line headline describing the provided text:

**MoEs: Efficiently pretraining and serving large language models with mixture-of-experts.**
Summary: Mixture of Experts (MoE) is a type of transformer model that uses sparsity to enable faster pretraining and inference compared to dense models. MoEs consist of sparse MoE layers, which have a certain number of "experts" (e.g. 8), where each expert is a neural network. A gate network or router determines which tokens are sent to which expert. MoEs have been used to train multi-trillion parameter models, such as the open-sourced 1.6T parameters Switch Transformers. Fine-tuning MoEs has historically been difficult due to overfitting, but recent work with MoE instruction-tuning has shown promise.

Link: https://huggingface.co/blog/moe

<img src="/img/ffe714a0-bb8c-4032-b597-ae9f3297a682.png" width="400" />


<sup><sub>1/13/2024 [Mark as read](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9882_0&tag=isread) [Mark as BestOf](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9882_0&tag=bestof) [Experiments](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9882_0&tag=Experiments)<sub/><sup/>

<br/><br/>

## Portkey's AI Gateway: Access 100+ LLMs with Unified API
Summary: Portkey's AI Gateway functions as an interface between applications and hosted Large Language Models, enabling streamlined API requests to various providers. It features a unified API signature for over 100 LLMs, allowing developers to connect using the OpenAI API signature without code modifications. Additional features include automatic retries, fallbacks, load balancing, and multiple SDKs for easy integration. Configurable routing strategies offer customization for fallbacks, retries, and load balancing.

Link: https://github.com/Portkey-AI/gateway

<img src="/img/9010a57f-f609-41d0-984d-09bda68172c1.png" width="400" />


<sup><sub>1/13/2024 [Mark as read](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9877_0&tag=isread) [Mark as BestOf](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9877_0&tag=bestof) [Experiments](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9877_0&tag=Experiments)<sub/><sup/>

<br/><br/>

## New tool converts HTML directories to Markdown for LLMs and RAG datasets
Summary: Clipper, a Node.js command-line tool, facilitates the conversion of HTML content to Markdown format, enabling simplified data extraction, web page crawling, and RAG dataset building. With easy installation through NPM, Clipper offers JSON output options, supports URL and file inputs, and offers features such as context trimming and output filtering. It streamlines the process of collecting and converting online content for various applications, including LLM training and RAG pipeline integration.

Link: https://www.linkedin.com/posts/philipp-schmid-a6a2bb196_just-released-an-update-to-clipper-you-can-activity-7150589245059977217-AiUN?utm_source=share&amp;utm_medium=member_android

<img src="/img/646fe201-ed8d-4949-b03a-0766b754adde.png" width="400" />


<sup><sub>1/12/2024 [Mark as read](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9870_0&tag=isread) [Mark as BestOf](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9870_0&tag=bestof) [Experiments](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9870_0&tag=Experiments)<sub/><sup/>

<br/><br/>

## Mixtral: A Sparse Mixture of Experts Language Model that Outperforms Llama 2 70B and GPT-3.5
Summary: Researchers released Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model, which outperforms Llama 2 70B and GPT-3.5 across all evaluated benchmarks. It has the same architecture as Mistral 7B, but each layer consists of 8 feedforward blocks (experts). A router network selects two experts for each token at each layer, combining their outputs. Despite seeing only two at a time, Mixtral effectively utilizes 47B parameters, with active parameters of 13B during inference. It was trained with a context size of 32k tokens and achieved impressive results. Additionally, a fine-tuned version, Mixtral 8x7B - Instruct, surpassed GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B - chat model on human benchmarks. Both models are released under the Apache 2.0 license.

Link: https://arxiv.org/abs/2401.04088?utm_source=aitidbits.substack.com&utm_medium=newsletter

<img src="/img/c5dc1189-7592-40ef-9ed9-79813baaff1d.png" width="400" />


<sup><sub>1/11/2024 [Mark as read](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9864_0&tag=isread) [Mark as BestOf](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9864_0&tag=bestof) [Experiments](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9864_0&tag=Experiments)<sub/><sup/>

<br/><br/>

## OpenChat: Open-source Language Model Advancing with Imperfect Data
Summary: OpenChat is an easy-to-use, open-source library of language models fine-tuned with C-RLFT, a strategy inspired by offline reinforcement learning. It learns from mixed-quality data without preference labels, runs on consumer GPUs, and outperforms ChatGPT and Grok-1 in various benchmarks.

Link: https://github.com/imoneoi/openchat?tab=readme-ov-file

<img src="/img/50895a6c-24b8-4e24-970d-17572b03ba2f.png" width="400" />


<sup><sub>1/11/2024 [Mark as read](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9862_0&tag=isread) [Mark as BestOf](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9862_0&tag=bestof) [Experiments](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9862_0&tag=Experiments)<sub/><sup/>

<br/><br/>

## DPO: A New Method to Align Language Models with Human Preferences
Summary: The Direct Preference Optimization (DPO) paper by Rafael Rafailov and others introduces a new approach to aligning language models with human preferences. This approach streamlines the training process by directly integrating the reward function with the language model training, eliminating the need for a separate reward model. DPO has the potential to simplify the alignment process and make it more efficient, potentially revolutionizing the way language models are trained and aligned with human preferences.

Link: https://www.linkedin.com/posts/andrewyng_ai-discovers-new-antibiotics-openai-revamps-activity-7151282706947969025-WV2v?utm_source=share&amp;utm_medium=member_android

<img src="/img/c4e93447-19fa-4ae8-a788-887f212b4420.png" width="400" />


<sup><sub>1/11/2024 [Mark as read](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9860_0&tag=isread) [Mark as BestOf](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9860_0&tag=bestof) [Experiments](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9860_0&tag=Experiments)<sub/><sup/>

<br/><br/>

## Explore standalone use cases of txtai embeddings components
Summary: The main components of txtai are embeddings, pipeline, workflow, and an API. Its package provides the glue between these components, making everything easy to use. Each of the packages is modular and can be used on its own. Embeddings package provides the glue between components, making everything easy to use.

Link: https://neuml.hashnode.dev/embeddings-index-components

<img src="/img/dcbde876-dd30-41cb-9464-9104b3554d31.png" width="400" />


<sup><sub>1/11/2024 [Mark as read](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9856_0&tag=isread) [Mark as BestOf](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9856_0&tag=bestof) [Experiments](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9856_0&tag=Experiments)<sub/><sup/>

<br/><br/>

## Blending Is All You Need: Smaller Models Can Rival or Even Outperform Larger Language Models
Summary: Researchers introduce a cost-effective alternative to large language models (LLMs) by demonstrating that blending multiple smaller models can achieve comparable or even superior performance to a single large model. This "blending" approach involves integrating multiple chat AIs, and empirical evidence suggests that when specific smaller models are synergistically combined, they can rival or surpass the capabilities of much larger counterparts. Rigorous A/B testing with a large user base confirms the effectiveness of blending, highlighting its potential as a viable strategy for enhancing chat AI efficacy without a corresponding surge in computational demands.

Link: https://arxiv.org/abs/2401.02994

<img src="/img/d92ca433-583a-47a1-926c-36a8792b6619.png" width="400" />


<sup><sub>1/9/2024 [Mark as read](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9842_0&tag=isread) [Mark as BestOf](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9842_0&tag=bestof) [Experiments](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9842_0&tag=Experiments)<sub/><sup/>

<br/><br/>

## Improved Latent Space Representation with Variational Autoencoders
Summary: Variational AutoEncoders (VAEs) are an extension of classical autoencoders typically used for dimensionality reduction. While classical autoencoders only minimize the reconstruction loss, VAEs instead maximize a lower bound on the log-likelihood of the data. This results in a more continuous and centralized latent space, which is advantageous for generative tasks. The posterior distribution in VAEs is approximated by a diagonal Gaussian distribution with parameters \(\mu\) and \(\sigma\), and the KL divergence between this distribution and the standard Gaussian is used as a penalty in the loss function. The resulting latent space is more compact and smooth, allowing for interpolation between input images and other fun applications.

Link: https://avandekleut.github.io/vae/

<img src="/img/66a15480-74d5-46e4-b88c-d4d063bbc644.png" width="400" />


<sup><sub>1/9/2024 [Mark as read](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9835_0&tag=isread) [Mark as BestOf](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9835_0&tag=bestof) [Experiments](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9835_0&tag=Experiments)<sub/><sup/>

<br/><br/>

## A Survey on Generative Information Extraction Methods Using Large Language Models
Summary: A survey on leveraging generative large language models (LLMs) for information extraction (IE) tasks is presented. The achievements of LLM-based IE methods are categorized, reviewed, and analyzed. The study provides insights into techniques and suggests promising research directions for future exploration. A public repository with consistently updated resources is maintained to facilitate ongoing research in this area.

Link: https://arxiv.org/abs/2312.17617v1

<img src="/img/fce3f106-83d0-40c6-ada2-89dfec55272c.png" width="400" />


<sup><sub>1/8/2024 [Mark as read](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9818_0&tag=isread) [Mark as BestOf](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9818_0&tag=bestof) [Experiments](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9818_0&tag=Experiments)<sub/><sup/>

<br/><br/>

## JPMorgan AI Research Introduces DocLLM: A Lightweight Extension to Traditional Large Language Models for Generative Reasoning Over Documents with Rich Layouts
Summary: JPMorgan AI Research presents DocLLM, an extension of large language models designed for reasoning over visual documents. It combines textual semantics and spatial layout using bounding box coordinates, allowing efficient cross-modal interaction capture. The model is pre-trained with a modified self-supervised target addressing layout issues and fine-tuned with instruction data for tasks like form comprehension, table alignment, and visual question answering, showing significant performance gains.

Link: https://www.marktechpost.com/2024/01/05/jpmorgan-ai-research-introduces-docllm-a-lightweight-extension-to-traditional-large-language-models-tailored-for-generative-reasoning-over-documents-with-rich-layouts/?amp=

<img src="/img/e3156297-1b6f-4fe0-b942-2483d95d261c.png" width="400" />


<sup><sub>1/6/2024 [Mark as read](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9805_0&tag=isread) [Mark as BestOf](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9805_0&tag=bestof) [Experiments](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9805_0&tag=Experiments)<sub/><sup/>

<br/><br/>

## JPMorgan AI Research Introduces DocLLM, a Lightweight Extension to Traditional Large Language Models for Generative Reasoning Over Documents with Rich Layouts
Summary: JPMorgan AI Research has introduced DocLLM, a lightweight extension to traditional Large Language Models (LLMs) specifically tailored for generative reasoning over documents with rich layouts. DocLLM represents both text semantics and spatial layouts, leveraging bounding box coordinates acquired through optical character recognition (OCR) to add spatial layout information. It extends the self-attention mechanism of transformers to capture cross-modal interactions between text and layout. DocLLM has demonstrated significant performance gains on various document intelligence tasks, including form comprehension, table alignment, visual question answering, and key information extraction, highlighting its effectiveness in handling complex document structures and mixed data types.

Link: https://www.marktechpost.com/2024/01/05/jpmorgan-ai-research-introduces-docllm-a-lightweight-extension-to-traditional-large-language-models-tailored-for-generative-reasoning-over-documents-with-rich-layouts/?amp=

<img src="/img/c5ac700d-366a-404f-a3e3-51dbeaa6f74e.png" width="400" />


<sup><sub>1/6/2024 [Mark as read](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9805_0&tag=isread) [Mark as BestOf](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9805_0&tag=bestof) [Experiments](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9805_0&tag=Experiments)<sub/><sup/>

<br/><br/>

## Curiosity, effort, ability, and luck are key factors in achieving great work. One should focus on something exciting that provides scope for great and unique work. It is recommended to choose a field, learn enough to reach the frontier of knowledge, notice gaps, and then explore promising ones.
Summary: This text describes the common traits of exceptionally productive people. The author asserts that the first step to excelling in a field is to choose a field that aligns with your natural aptitudes, interests, and offers ample opportunities for groundbreaking work. However, finding such a field is difficult, especially when you're young. Therefore, the author recommends experimenting with different fields and projects until you find one that truly excites you.

Once you've chosen a field, the next step is to learn as much as you can about it and identify knowledge gaps. Then, focus on filling those gaps by conducting research, asking questions, and seeking out experts in the field. The author emphasizes the importance of embracing strange or unconventional ideas, as these often lead to breakthroughs.

The author also stresses the significance of perseverance and hard work. Great work, they argue, often requires long hours and intense focus. However, it is important to avoid burnout by taking breaks and engaging in activities that recharge your energy.

To ensure consistency in your work, the author suggests setting clear goals and creating a schedule that allows for uninterrupted periods of focused work. Additionally, they recommend avoiding distractions and interruptions, both during work and during breaks.

To improve your work further, the author encourages seeking feedback from others, especially those who are knowledgeable in your field. Constructive criticism can help you identify areas where you can improve and refine your work.

Finally, the author highlights the importance of maintaining a curious and open mindset. By continuously seeking new knowledge and experiences, you can expand your understanding of the world and generate innovative ideas.

Link: https://paulgraham.com/greatwork.html

<img src="/img/46e14cdf-da78-48e6-90bb-10c9a4483a03.png" width="400" />


<sup><sub>12/28/2023 [Mark as read](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9657_0&tag=isread) [Mark as BestOf](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9657_0&tag=bestof) [Experiments](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9657_0&tag=Experiments)<sub/><sup/>

<br/><br/>

## A List of Open-Source LLMs for Builders
Summary: This article presents a comprehensive list of open-source Large Language Models (LLMs) available for both commercial and research purposes. It includes models like Flan-U\u00b2, OpenChatKit, Cerebras-GPT, Pythia, Bloom & mTO, OpenAssistant, nanoT5, GeoV, Baize, Vicuna, Koala, GPT4All, Lit-LLaMA, Dolly, Dalai, Alpaca.cpp, Alpaca-LORA, and llama.cpp. While most of these models can be used commercially, Lit-LLaMA, Dolly, and OpenAssistant's offerings are restricted to non-commercial use. The article emphasizes that users should carefully consider their intended use case when selecting a model, as many require instruction tuning to perform effectively.

Link: https://www.linkedin.com/feed/update/urn:li:activity:7049789761728770049?utm_source=share&utm_medium=member_android

<img src="/img/ccc7d6d4-ae0a-4d05-9334-9fe130ed51dd.png" width="400" />


<sup><sub>4/8/2023 [Mark as read](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=8221_0&tag=isread) [Mark as BestOf](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=8221_0&tag=bestof) [Experiments](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=8221_0&tag=Experiments)<sub/><sup/>

<br/><br/>

