## Calendars Can Be Time Machines: Exploring New Layers and Dimensional Time Management
Summary: Calendars, essential tools for time management, have remained largely static despite technological advancements. They fail to distinguish between different event types, such as tasks, meetings, and blocked time, and lack the ability to integrate data from other sources. This essay proposes introducing native layers to calendars that would allow for the seamless integration of various activities, including Spotify listening history, sleep quality data, and personal activities from the past. By enhancing calendars' capabilities to accommodate diverse data layers, they can transform into actual time machines, enabling users to shape the future through insights gained from past experiences and unlock a range of new productivity use cases.

Link: https://julian.digital/2023/07/06/multi-layered-calendars/

<img src="/img/8fa53570-9c8e-4f96-ad96-c3e6ccf63472.png" width="400" />


<sup><sub>3/6/2024 [Mark as read](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=10205_0&tag=isread) [Mark as BestOf](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=10205_0&tag=bestof) [Experiments](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=10205_0&tag=Experiments)<sub/><sup/>

<br/><br/>

## StructLM: Advancing LLMs for Structured Knowledge Grounding
Summary: The StructLM project aims to enhance Structured Knowledge Grounding (SKG) capabilities in Large Language Models (LLMs). Researchers created a comprehensive instruction tuning dataset and trained a series of models (StructLM-7B to StructLM-34B) based on the Code-LLaMA architecture. StructLM excels on 14 out of 18 evaluated SKG datasets, setting new state-of-the-art achievements on 7 tasks. However, researchers found that increasing model size beyond StructLM-7B offered marginal benefits, suggesting that advancing SKG требует more innovative approaches.

Link: https://tiger-ai-lab.github.io/StructLM/

<img src="/img/f82bef1d-0a32-4c95-8b00-2ac184deb024.png" width="400" />


<sup><sub>3/3/2024 [Mark as read](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=10189_0&tag=isread) [Mark as BestOf](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=10189_0&tag=bestof) [Experiments](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=10189_0&tag=Experiments)<sub/><sup/>

<br/><br/>

## Getting Started with DSPy: A Comprehensive Guide to LLM Programming
Summary: This video tutorial by Connor Shorten introduces the DSPy programming language, which simplifies the creation of language models by compiling declarative calls into pipelines. The video covers key aspects of DSPy development, including installation, data management with dspy.Example, LLM metrics, the DSPy programming model, and optimization techniques, making it a comprehensive resource for getting started with DSPy programming.

Link: https://youtu.be/CEuUG4Umfxs?si=86Gp7CKV3PuBzpds

<img src="/img/1818d5d3-b3db-4532-9872-402b4a63745c.png" width="400" />


<sup><sub>2/29/2024 [Mark as read](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=10178_0&tag=isread) [Mark as BestOf](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=10178_0&tag=bestof) [Experiments](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=10178_0&tag=Experiments)<sub/><sup/>

<br/><br/>

## Fireworks introduces FireFunction-v1, an open-weights, GPT-4-level function calling model for faster and accurate decision-making.
Summary: Fireworks released FireFunction-v1, a new and improved open-weights model based on Mixtral. This model offers several advantages over its predecessor, including higher accuracy for real-world use cases, improved response accuracy for multilingual inputs, and the ability to configure "tool_choice" to 'any' to force a function call. It also outperforms other OSS models in terms of speed and accuracy and is available for free during a limited beta period. Additionally, it supports structured output generation and routing decision-making, making it a versatile tool for developers building LLM applications.

Link: https://fireworks.ai/blog/firefunction-v1-gpt-4-level-function-calling

<img src="/img/a4ef1d51-980c-45ef-80da-27b49674adfa.png" width="400" />


<sup><sub>2/25/2024 [Mark as read](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=10155_0&tag=isread) [Mark as BestOf](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=10155_0&tag=bestof) [Experiments](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=10155_0&tag=Experiments)<sub/><sup/>

<br/><br/>

## Demystifying Transformers: Unveiling the Mechanisms of ChatGPT, GPT-4, and LLaMa
Summary: Transformers are a type of neural network architecture that has become increasingly popular for natural language processing tasks. They are used in a variety of applications, including machine translation, text summarization, and question answering. Transformers learn context and relationships in sequential data through an attention mechanism, allowing them to understand the meaning of words and phrases in context. At inference time, Transformers use a stack of encoder and decoder layers to generate text or translate languages, while during training, these layers are optimized using backpropagation to minimize a loss function.

Link: https://www.youtube.com/watch?v=IGu7ivuy1Ag

<img src="/img/b3db696d-c00c-4398-bebc-ca825c2f8960.png" width="400" />


<sup><sub>2/22/2024 [Mark as read](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=10140_0&tag=isread) [Mark as BestOf](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=10140_0&tag=bestof) [Experiments](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=10140_0&tag=Experiments)<sub/><sup/>

<br/><br/>

## Introducing NVIDIA Chat with RTX: A Personalized AI Chatbot on Your RTX PC
Summary: NVIDIA Chat with RTX is a demo app that integrates a large language model with user-provided content to create a personalized chatbot. It utilizes retrieval-augmented generation and RTX acceleration to provide fast and relevant answers to queries. Users can input text documents, PDFs, URLs, and YouTube playlists into the app's library, and developers can access the underlying technology through the TensorRT-LLM RAG reference project on GitHub. Chat with RTX is available for Windows PCs and workstations with NVIDIA RTX GPUs and requires Windows 11, an RTX 30 or 40 Series GPU with at least 8GB of VRAM, and 16GB or more RAM.

Link: https://www.nvidia.com/en-us/ai-on-rtx/chat-with-rtx-generative-ai/

<img src="/img/3eeaeac7-7a77-4cd3-b47e-6e05b40c498b.png" width="400" />


<sup><sub>2/20/2024 [Mark as read](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=10118_0&tag=isread) [Mark as BestOf](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=10118_0&tag=bestof) [Experiments](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=10118_0&tag=Experiments)<sub/><sup/>

<br/><br/>

## Surya: Document OCR Toolkit Featuring Accurate OCR and Line-Level Text Detection
Summary: Surya is an OCR toolkit that accurately extracts text and lines from documents in over 90 languages. It outperforms Tesseract in terms of speed, accuracy, and coverage. Surya is designed for OCR on printed text but may work on some handwritten images. It supports text extraction from images, PDFs, and folders and can also visualize the detected text lines.

Link: https://github.com/VikParuchuri/surya

<img src="/img/10e86b52-4ff6-4910-a6d8-34d0afa16104.png" width="400" />


<sup><sub>2/19/2024 [Mark as read](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=10115_0&tag=isread) [Mark as BestOf](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=10115_0&tag=bestof) [Experiments](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=10115_0&tag=Experiments)<sub/><sup/>

<br/><br/>

## GitHub: A comprehensive platform for developers
Summary: GitHub is a platform for hosting and collaborating on code projects. It provides a range of features, including code repositories, issue tracking, and project management tools. GitHub offers support for developers through its Docs, Community Forum, and Professional Services, as well as a subscriber-based developer newsletter featuring technical guides and industry best practices.

Link: https://github.com/ray-project/llf-board

<img src="/img/6df1d5d8-9a40-4901-9453-a92358cc0933.png" width="400" />


<sup><sub>2/19/2024 [Mark as read](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=10113_0&tag=isread) [Mark as BestOf](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=10113_0&tag=bestof) [Experiments](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=10113_0&tag=Experiments)<sub/><sup/>

<br/><br/>

## Groq: Offering API Access to the World's Fastest Open-Source LLM Inference Engine
Summary: Groq provides API access to approved members, offering the fastest inference speed for open-source LLMs, including Llama 2 70B, with a 10-day free trial. Groq guarantees the most competitive pricing and offers additional models like Mistral and CodeLlama upon request. Their LPU Inference Engine boasts an 18x faster LLM inference performance compared to cloud providers, as demonstrated on Anyscale's LLMPerf Leaderboard.

Link: https://wow.groq.com/

<img src="/img/56ff21cb-ede1-4b7d-aa59-4a47582bfdc5.png" width="400" />


<sup><sub>2/19/2024 [Mark as read](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=10107_0&tag=isread) [Mark as BestOf](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=10107_0&tag=bestof) [Experiments](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=10107_0&tag=Experiments)<sub/><sup/>

<br/><br/>

## Hugging Face Researchers Train One of the Largest Context Size Transformers on Long Video and Language Sequences
Summary: Researchers have developed a large transformer model that can process over 1 million tokens of video and language sequences. The model, trained on a massive dataset using the RingAttention technique, improves long sequence understanding and retrieval tasks. The authors also provide open-sourced 7B parameter models for text and video processing, addressing challenges such as memory constraints, computational complexity, and data limitations. This work enables broader AI capabilities for assisting humans by combining human textual knowledge with the physical world's understanding.

Link: https://huggingface.co/papers/2402.08268

<img src="/img/3f867aaf-26a2-47a5-9061-c1fe77a06a78.png" width="400" />


<sup><sub>2/17/2024 [Mark as read](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=10103_0&tag=isread) [Mark as BestOf](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=10103_0&tag=bestof) [Experiments](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=10103_0&tag=Experiments)<sub/><sup/>

<br/><br/>

## Microsoft Copilot for Microsoft 365: An AI-powered tool that enhances productivity by coordinating AI models with Microsoft 365 apps.
Summary: Copilot for Microsoft 365 is an AI-powered tool that uses LLMs, content from Microsoft Graph, and various apps to assist users. It uses pre-trained models like GPT-4 to generate text and content, coordinate tasks, and generate responses. To function, it receives an input prompt from a user, processes it through "grounding" to make answers more specific and actionable, and sends it to the LLM for processing. The response from the LLM is then post-processed, reviewed by the responsible AI team, and sent back to the app for review by the user.

Link: https://www.linkedin.com/posts/jackrowbotham_microsoft-technology-microsoft365-activity-7161599604705218561-p9Ye?utm_source=share&amp;utm_medium=member_android

<img src="/img/f4daa089-ddc8-4047-a099-49a8f00823ab.png" width="400" />


<sup><sub>2/10/2024 [Mark as read](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=10064_0&tag=isread) [Mark as BestOf](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=10064_0&tag=bestof) [Experiments](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=10064_0&tag=Experiments)<sub/><sup/>

<br/><br/>

## MetaVoice-1B: A Powerful 1.2B Parameter Base Model for Text-to-Speech
Summary: MetaVoice-1B is a 1.2B parameter text-to-speech model with a focus on emotional speech rhythm and tone in English, support for voice cloning with finetuning, and zero-shot cloning for American and British voices. It predicts EnCodec tokens from text and speaker information and uses a causal GPT to predict the first two hierarchies of EnCodec tokens. The rest of the 6 hierarchies are predicted using a non-causal transformer. Multi-band diffusion is used to generate waveforms from the EnCodec tokens, and DeepFilterNet is used to clean up artifacts introduced by the diffusion process.

Link: https://huggingface.co/metavoiceio/metavoice-1B-v0.1

<img src="/img/48b63e33-12aa-4568-8697-d9ec42db232b.png" width="400" />


<sup><sub>2/7/2024 [Mark as read](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=10050_0&tag=isread) [Mark as BestOf](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=10050_0&tag=bestof) [Experiments](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=10050_0&tag=Experiments)<sub/><sup/>

<br/><br/>

## MetaVoice 1B: A 1.2 Billion Parameter Voice Cloning Model Released with Apache 2.0 License
Summary: MetaVoice 1B is a new text-to-speech (TTS) model released by MetaVoice, with 1.2 billion parameters and trained on 100K hours of data. It can do zero-shot voice cloning, short and long-form synthesis, and can portray emotions in speech. The model is Apache 2.0 licensed and its architecture consists of an encoder (Multi-Band Diffusion), a GPT + Encoder Transformer LM, and a DeepFilterNet to clean up artifacts.

Link: https://www.linkedin.com/posts/vaibhavs10_lets-go-metavoice-1b-by-metavoice-ugcPost-7161031611092733953-tTcT?utm_source=share&utm_medium=member_android

<img src="/img/f5bb5532-ed69-423e-bce5-b0559f339fab.png" width="400" />


<sup><sub>2/7/2024 [Mark as read](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=10048_0&tag=isread) [Mark as BestOf](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=10048_0&tag=bestof) [Experiments](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=10048_0&tag=Experiments)<sub/><sup/>

<br/><br/>

## Meta releases Code Llama 70B, a large language model achieving human-level performance on code generation tasks.
Summary: Meta released Code Llama 70B, the largest version of Code Llama, an AI system designed for programming tasks. It has achieved 67.8% on the HumanEval benchmark, matching the initial performance of the recently-hyped GPT-4. Code Llama 70B is initialized from Llama 2, trained on 1T Tokens, and fine-tuned on Python and Instruct versions. It has a context window of 16384 and is available on Hugging Face, with plans to integrate it into Hugging Chat soon.

Link: https://www.linkedin.com/posts/philipp-schmid-a6a2bb196_code-llama-70b-is-heremeta-just-activity-7157780231116738562-GlmI?utm_source=share&amp;utm_medium=member_android

<img src="/img/f0b117f2-83fe-4a0b-a3c3-6135c48c2e52.png" width="400" />


<sup><sub>1/29/2024 [Mark as read](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=10008_0&tag=isread) [Mark as BestOf](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=10008_0&tag=bestof) [Experiments](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=10008_0&tag=Experiments)<sub/><sup/>

<br/><br/>

## Open Source Text-to-Speech System Built by Inverting Whisper
Summary: WhisperSpeech is an open-source text-to-speech system built on top of Whisper, EnCodec, and Vocos models. The system uses Whisper to generate semantic tokens from audio, EnCodec to model acoustic tokens, and Vocos to generate high-quality audio from EnCodec tokens. WhisperSpeech is designed to be powerful and easily customizable, and it is currently trained on the English LibreLight dataset. Future releases will target multiple languages.

Link: https://github.com/collabora/WhisperSpeech

<img src="/img/68fbfb67-1368-44aa-b57f-9618fbec8c0a.png" width="400" />


<sup><sub>1/24/2024 [Mark as read](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9969_0&tag=isread) [Mark as BestOf](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9969_0&tag=bestof) [Experiments](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9969_0&tag=Experiments)<sub/><sup/>

<br/><br/>

## Colab Notebook Runs Mixtral8x7B-Instruct Model with MoE Offloading for Text Generation
Summary: The provided text uses a pre-trained quantized model, Mixtral-8x7B-Instruct, implemented with a mixed-precision strategy and a specific offloading approach, making it suitable for inference on consumer-grade GPUs or in Google Colab with limited VRAM and RAM resources. It outlines the process of setting up the environment, initializing the model, and generating text sequences interactively, demonstrating its language generation capabilities. The text also includes a couple of funny poems in response to user prompts, showcasing the model's ability to generate creative text based on user input.

Link: https://colab.research.google.com/github/dvmazur/mixtral-offloading/blob/master/notebooks/demo.ipynb#scrollTo=f7qY7ebqX7T7

<img src="/img/a1ccacca-62c9-4256-85fa-d60954da19e7.png" width="400" />


<sup><sub>1/20/2024 [Mark as read](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9932_0&tag=isread) [Mark as BestOf](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9932_0&tag=bestof) [Experiments](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9932_0&tag=Experiments)<sub/><sup/>

<br/><br/>

## New programming language optimizes prompts for large language models
Summary: LMQL, a programming language designed specifically for interacting with Large Language Models (LLMs), has just released version 0.7. It enables robust and modular LLM prompting with features such as types, templates, constraints, and an optimizing runtime. Programmers can write queries in LMQL, which are then sent to an LLM to generate responses. The results are directly accessible, enabling the construction of complex prompts with guaranteed output formats. Moreover, LMQL supports nested queries, modularized local instructions, and re-use of prompt components, making it a powerful tool for prompt engineering.

Link: https://lmql.ai/

<img src="/img/4ed56468-4916-4da4-b17e-a90567bb6ce7.png" width="400" />


<sup><sub>1/20/2024 [Mark as read](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9930_0&tag=isread) [Mark as BestOf](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9930_0&tag=bestof) [Experiments](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9930_0&tag=Experiments)<sub/><sup/>

<br/><br/>

## Using External Data, Pinecone Serverless Improves Large Language Models
Summary: The research presented demonstrates that using Retrieval-Augmented Generation (RAG) significantly improves the quality of responses generated by Large Language Models (LLMs), such as GPT-4 and Llama 2, even when dealing with questions that fall within the LLM's training domain. As more data is integrated into the RAG system, the performance of LLMs improves, with a remarkable 13% enhancement in the faithfulness of answers observed in GPT-4 when supplemented with RAG over a billion-record corpus. Furthermore, the study highlights that different LLMs, regardless of their size or complexity, can achieve comparable levels of accuracy and reliability when combined with RAG, democratizing access to state-of-the-art generative AI capabilities.

Link: https://www.pinecone.io/blog/rag-study/

<img src="/img/480f3a6f-3964-4cdd-8d2a-fc017c4e3aec.png" width="400" />


<sup><sub>1/19/2024 [Mark as read](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9924_0&tag=isread) [Mark as BestOf](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9924_0&tag=bestof) [Experiments](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9924_0&tag=Experiments)<sub/><sup/>

<br/><br/>

## Composable Retrievers: A New Way to Link Retrieval Models for Advanced RAG Pipelines
Summary: LlamaIndex introduces a composable retriever interface that enables linking any retriever to any other retriever or RAG pipeline. This simplification allows users to define arbitrary hierarchies of retrievers and custom retrievers easily. The interface offers several benefits, including simplified writing of advanced RAG interfaces, creation of arbitrary hierarchies of retrievers, and definition of custom retrievers.

Link: https://www.linkedin.com/posts/llamaindex_composable-retrieval-a-lot-of-advanced-activity-7153785306419216384-wjsr?utm_source=share&amp;utm_medium=member_android

<img src="/img/f87ffb82-2111-4b1b-8bc8-5d529e532bce.png" width="400" />


<sup><sub>1/19/2024 [Mark as read](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9920_0&tag=isread) [Mark as BestOf](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9920_0&tag=bestof) [Experiments](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9920_0&tag=Experiments)<sub/><sup/>

<br/><br/>

## Sure, here's a one-line headline describing the text you provided:

**Government Unveils Plan to Combat Physician Burnout and Improve Patient Care**
Summary: I do not have access to external websites or specific documents, including the text you are referring to. Therefore, I am unable to summarize the text for you.

Link: https://www.wsj.com/tech/samsung-galaxy-s24-artificial-intelligence-features-dc37e134?mod=tech_trendingnow_article_pos1

<img src="/img/29de4cd2-f593-4da1-847f-a5a984f00b99.png" width="400" />


<sup><sub>1/18/2024 [Mark as read](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9916_0&tag=isread) [Mark as BestOf](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9916_0&tag=bestof) [Experiments](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9916_0&tag=Experiments)<sub/><sup/>

<br/><br/>

## Hugging Face multimodal team releases Websight, a dataset of screenshots and HTML/CSS code for training Vision Language Models.
Summary: Hugging Face released Websight, a dataset of 823,000 website screenshots and HTML/CSS code for training Vision Language Models (VLMs) to convert images to code. The dataset is generated using open models and can be used commercially. A fine-tuned open model is also available for free on Hugging Face.

Link: https://www.linkedin.com/posts/philipp-schmid-a6a2bb196_transform-screenshots-into-html-code-effortlessly-activity-7152949221824872449-62iJ?utm_source=share&amp;utm_medium=member_desktop

<img src="/img/eb0d2ae7-8591-4ef9-933b-8b62eb6f3acb.png" width="400" />


<sup><sub>1/16/2024 [Mark as read](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9905_0&tag=isread) [Mark as BestOf](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9905_0&tag=bestof) [Experiments](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9905_0&tag=Experiments)<sub/><sup/>

<br/><br/>

## From scratch implementation of Self-Attention, Multi-Head Attention, Cross-Attention, and Causal Self-Attention in Large Language Models (LLMs)
Summary: This article explains the inner workings of the self-attention mechanism, a core component of large language models (LLMs) like GPT-4 and Llama, through a step-by-step coding approach. It also covers multi-head attention, cross-attention, and causal self-attention.

Link: https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention

<img src="/img/fc744586-928d-47fd-bd8e-87045144ad19.png" width="400" />


<sup><sub>1/15/2024 [Mark as read](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9895_0&tag=isread) [Mark as BestOf](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9895_0&tag=bestof) [Experiments](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9895_0&tag=Experiments)<sub/><sup/>

<br/><br/>

## This article lists recommended books of various genres, all of which promise to expand readers' minds.
Summary: This repository contains a list of mind-expanding books curated by various contributors. The selection covers a wide range of topics, including startups and business, philosophy and psychology, autobiographies and biographies, history, science and medicine, logic and problem-solving, politics, economics, gender, sexuality, race, education, writing, theater and film, Shakespeare, fiction, and miscellaneous subjects like health, design, travel, language, nature, and art. Some popular book recommendations are Shoe Dog by Phil Knight, The Ride of a Lifetime by Robert Iger, and Bad Blood by John Carreyrou.

Link: https://github.com/hackerkid/Mind-Expanding-Books

<img src="/img/9e8c5f7b-a2eb-403f-8998-875c15b5938d.png" width="400" />


<sup><sub>1/15/2024 [Mark as read](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9893_0&tag=isread) [Mark as BestOf](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9893_0&tag=bestof) [Experiments](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9893_0&tag=Experiments)<sub/><sup/>

<br/><br/>

## Automatic Evaluation Framework for Assessing LLMs' Protocol Planning Abilities in Biology
Summary: The paper presents a novel automatic evaluation framework and dataset called BioProt for assessing the performance of Large Language Models (LLMs) in planning experimental protocols in biology. The framework involves converting natural language protocols into pseudocode representations, which enables the evaluation of an LLM's ability to reconstruct the pseudocode from high-level descriptions and admissible pseudocode functions. The study explores the performance of GPT-3 and GPT-4 on this task and examines their robustness. It also demonstrates the utility of pseudocode representations by generating accurate novel protocols and successfully executing a generated protocol in a biological laboratory. The extensibility of the framework to other areas of science or domains lacking automatic evaluation is highlighted.

Link: https://arxiv.org/abs/2310.10632?utm_source=substack&utm_medium=email

<img src="/img/38d1a3b5-34b5-4974-bf30-dc849cad863c.png" width="400" />


<sup><sub>1/15/2024 [Mark as read](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9891_0&tag=isread) [Mark as BestOf](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9891_0&tag=bestof) [Experiments](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9891_0&tag=Experiments)<sub/><sup/>

<br/><br/>

## Explore topics, data connectivity and run network analysis with the Semantic Graph
Summary: A semantic graph, also known as a knowledge graph or semantic network, is introduced, constructed with semantic relationships connecting the nodes. Nodes and relationships can be added to the graph, and analysis functions can be run on it. Semantic graphs can be used to explore relationships, such as topics and interconnections in a dataset. Embeddings instances can be indexed into a graph, allowing for network analysis and topic modeling. Topic modeling can be done using community detection algorithms, and centrality and pagerank can be used to analyze the graph. The graph can also be traversed to show how nodes are connected. Furthermore, images can be grouped into topics using topic modeling, and the image graph can be walked to explore relationships between images.

Link: https://neuml.hashnode.dev/introducing-the-semantic-graph

<img src="/img/b9351921-70a5-4365-aec2-52c9c1ba5b74.png" width="400" />


<sup><sub>1/15/2024 [Mark as read](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9889_0&tag=isread) [Mark as BestOf](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9889_0&tag=bestof) [Experiments](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9889_0&tag=Experiments)<sub/><sup/>

<br/><br/>

## Open-source, Highly Accurate Optical Character Recognition (OCR) System Released
Summary: A new open-source Optical Character Recognition (OCR) tool called Surya has been released, which accurately extracts text from images and supports multiple languages. It can recognize text at the line level, making it a valuable tool for tasks such as document processing and data extraction.

Link: https://www.linkedin.com/posts/alexcarliera_a-new-highly-accurate-ocr-was-just-released-activity-7151966210732040192-MeDq?utm_source=share&amp;utm_medium=member_android

<img src="/img/5a96af17-2a6c-45f1-8a69-663aa357620c.png" width="400" />


<sup><sub>1/14/2024 [Mark as read](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9884_0&tag=isread) [Mark as BestOf](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9884_0&tag=bestof) [Experiments](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9884_0&tag=Experiments)<sub/><sup/>

<br/><br/>

## Sure, here is a one-line headline describing the provided text:

**MoEs: Efficiently pretraining and serving large language models with mixture-of-experts.**
Summary: Mixture of Experts (MoE) is a type of transformer model that uses sparsity to enable faster pretraining and inference compared to dense models. MoEs consist of sparse MoE layers, which have a certain number of "experts" (e.g. 8), where each expert is a neural network. A gate network or router determines which tokens are sent to which expert. MoEs have been used to train multi-trillion parameter models, such as the open-sourced 1.6T parameters Switch Transformers. Fine-tuning MoEs has historically been difficult due to overfitting, but recent work with MoE instruction-tuning has shown promise.

Link: https://huggingface.co/blog/moe

<img src="/img/ffe714a0-bb8c-4032-b597-ae9f3297a682.png" width="400" />


<sup><sub>1/13/2024 [Mark as read](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9882_0&tag=isread) [Mark as BestOf](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9882_0&tag=bestof) [Experiments](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9882_0&tag=Experiments)<sub/><sup/>

<br/><br/>

## Portkey's AI Gateway: Access 100+ LLMs with Unified API
Summary: Portkey's AI Gateway functions as an interface between applications and hosted Large Language Models, enabling streamlined API requests to various providers. It features a unified API signature for over 100 LLMs, allowing developers to connect using the OpenAI API signature without code modifications. Additional features include automatic retries, fallbacks, load balancing, and multiple SDKs for easy integration. Configurable routing strategies offer customization for fallbacks, retries, and load balancing.

Link: https://github.com/Portkey-AI/gateway

<img src="/img/9010a57f-f609-41d0-984d-09bda68172c1.png" width="400" />


<sup><sub>1/13/2024 [Mark as read](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9877_0&tag=isread) [Mark as BestOf](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9877_0&tag=bestof) [Experiments](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9877_0&tag=Experiments)<sub/><sup/>

<br/><br/>

## New tool converts HTML directories to Markdown for LLMs and RAG datasets
Summary: Clipper, a Node.js command-line tool, facilitates the conversion of HTML content to Markdown format, enabling simplified data extraction, web page crawling, and RAG dataset building. With easy installation through NPM, Clipper offers JSON output options, supports URL and file inputs, and offers features such as context trimming and output filtering. It streamlines the process of collecting and converting online content for various applications, including LLM training and RAG pipeline integration.

Link: https://www.linkedin.com/posts/philipp-schmid-a6a2bb196_just-released-an-update-to-clipper-you-can-activity-7150589245059977217-AiUN?utm_source=share&amp;utm_medium=member_android

<img src="/img/646fe201-ed8d-4949-b03a-0766b754adde.png" width="400" />


<sup><sub>1/12/2024 [Mark as read](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9870_0&tag=isread) [Mark as BestOf](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9870_0&tag=bestof) [Experiments](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9870_0&tag=Experiments)<sub/><sup/>

<br/><br/>

## Mixtral: A Sparse Mixture of Experts Language Model that Outperforms Llama 2 70B and GPT-3.5
Summary: Researchers released Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model, which outperforms Llama 2 70B and GPT-3.5 across all evaluated benchmarks. It has the same architecture as Mistral 7B, but each layer consists of 8 feedforward blocks (experts). A router network selects two experts for each token at each layer, combining their outputs. Despite seeing only two at a time, Mixtral effectively utilizes 47B parameters, with active parameters of 13B during inference. It was trained with a context size of 32k tokens and achieved impressive results. Additionally, a fine-tuned version, Mixtral 8x7B - Instruct, surpassed GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B - chat model on human benchmarks. Both models are released under the Apache 2.0 license.

Link: https://arxiv.org/abs/2401.04088?utm_source=aitidbits.substack.com&utm_medium=newsletter

<img src="/img/c5dc1189-7592-40ef-9ed9-79813baaff1d.png" width="400" />


<sup><sub>1/11/2024 [Mark as read](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9864_0&tag=isread) [Mark as BestOf](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9864_0&tag=bestof) [Experiments](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9864_0&tag=Experiments)<sub/><sup/>

<br/><br/>

## OpenChat: Open-source Language Model Advancing with Imperfect Data
Summary: OpenChat is an easy-to-use, open-source library of language models fine-tuned with C-RLFT, a strategy inspired by offline reinforcement learning. It learns from mixed-quality data without preference labels, runs on consumer GPUs, and outperforms ChatGPT and Grok-1 in various benchmarks.

Link: https://github.com/imoneoi/openchat?tab=readme-ov-file

<img src="/img/50895a6c-24b8-4e24-970d-17572b03ba2f.png" width="400" />


<sup><sub>1/11/2024 [Mark as read](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9862_0&tag=isread) [Mark as BestOf](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9862_0&tag=bestof) [Experiments](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9862_0&tag=Experiments)<sub/><sup/>

<br/><br/>

## DPO: A New Method to Align Language Models with Human Preferences
Summary: The Direct Preference Optimization (DPO) paper by Rafael Rafailov and others introduces a new approach to aligning language models with human preferences. This approach streamlines the training process by directly integrating the reward function with the language model training, eliminating the need for a separate reward model. DPO has the potential to simplify the alignment process and make it more efficient, potentially revolutionizing the way language models are trained and aligned with human preferences.

Link: https://www.linkedin.com/posts/andrewyng_ai-discovers-new-antibiotics-openai-revamps-activity-7151282706947969025-WV2v?utm_source=share&amp;utm_medium=member_android

<img src="/img/c4e93447-19fa-4ae8-a788-887f212b4420.png" width="400" />


<sup><sub>1/11/2024 [Mark as read](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9860_0&tag=isread) [Mark as BestOf](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9860_0&tag=bestof) [Experiments](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9860_0&tag=Experiments)<sub/><sup/>

<br/><br/>

## Explore standalone use cases of txtai embeddings components
Summary: The main components of txtai are embeddings, pipeline, workflow, and an API. Its package provides the glue between these components, making everything easy to use. Each of the packages is modular and can be used on its own. Embeddings package provides the glue between components, making everything easy to use.

Link: https://neuml.hashnode.dev/embeddings-index-components

<img src="/img/dcbde876-dd30-41cb-9464-9104b3554d31.png" width="400" />


<sup><sub>1/11/2024 [Mark as read](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9856_0&tag=isread) [Mark as BestOf](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9856_0&tag=bestof) [Experiments](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9856_0&tag=Experiments)<sub/><sup/>

<br/><br/>

## Blending Is All You Need: Smaller Models Can Rival or Even Outperform Larger Language Models
Summary: Researchers introduce a cost-effective alternative to large language models (LLMs) by demonstrating that blending multiple smaller models can achieve comparable or even superior performance to a single large model. This "blending" approach involves integrating multiple chat AIs, and empirical evidence suggests that when specific smaller models are synergistically combined, they can rival or surpass the capabilities of much larger counterparts. Rigorous A/B testing with a large user base confirms the effectiveness of blending, highlighting its potential as a viable strategy for enhancing chat AI efficacy without a corresponding surge in computational demands.

Link: https://arxiv.org/abs/2401.02994

<img src="/img/d92ca433-583a-47a1-926c-36a8792b6619.png" width="400" />


<sup><sub>1/9/2024 [Mark as read](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9842_0&tag=isread) [Mark as BestOf](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9842_0&tag=bestof) [Experiments](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9842_0&tag=Experiments)<sub/><sup/>

<br/><br/>

## Improved Latent Space Representation with Variational Autoencoders
Summary: Variational AutoEncoders (VAEs) are an extension of classical autoencoders typically used for dimensionality reduction. While classical autoencoders only minimize the reconstruction loss, VAEs instead maximize a lower bound on the log-likelihood of the data. This results in a more continuous and centralized latent space, which is advantageous for generative tasks. The posterior distribution in VAEs is approximated by a diagonal Gaussian distribution with parameters \(\mu\) and \(\sigma\), and the KL divergence between this distribution and the standard Gaussian is used as a penalty in the loss function. The resulting latent space is more compact and smooth, allowing for interpolation between input images and other fun applications.

Link: https://avandekleut.github.io/vae/

<img src="/img/66a15480-74d5-46e4-b88c-d4d063bbc644.png" width="400" />


<sup><sub>1/9/2024 [Mark as read](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9835_0&tag=isread) [Mark as BestOf](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9835_0&tag=bestof) [Experiments](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9835_0&tag=Experiments)<sub/><sup/>

<br/><br/>

## A Survey on Generative Information Extraction Methods Using Large Language Models
Summary: A survey on leveraging generative large language models (LLMs) for information extraction (IE) tasks is presented. The achievements of LLM-based IE methods are categorized, reviewed, and analyzed. The study provides insights into techniques and suggests promising research directions for future exploration. A public repository with consistently updated resources is maintained to facilitate ongoing research in this area.

Link: https://arxiv.org/abs/2312.17617v1

<img src="/img/fce3f106-83d0-40c6-ada2-89dfec55272c.png" width="400" />


<sup><sub>1/8/2024 [Mark as read](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9818_0&tag=isread) [Mark as BestOf](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9818_0&tag=bestof) [Experiments](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9818_0&tag=Experiments)<sub/><sup/>

<br/><br/>

## Harvard Offers 10 Free Online Courses on Data Science, Statistics, and Web Programming
Summary: Harvard University is offering a variety of free data science, statistics, and web programming courses. These courses cover a wide range of topics, from high-dimensional data analysis to understanding technology. The courses are taught by experts in the field and are designed to be accessible to learners of all levels.

Link: https://www.linkedin.com/posts/lauradrahanbennett_ai-machinelearning-datascience-activity-7146321506380218368-1nnL?utm_source=share&amp;utm_medium=member_android

<img src="/img/627da72a-a633-4ccd-8c15-65b286f75368.png" width="400" />


<sup><sub>1/8/2024 [Mark as read](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9816_0&tag=isread) [Mark as BestOf](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9816_0&tag=bestof) [Experiments](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9816_0&tag=Experiments)<sub/><sup/>

<br/><br/>

## JPMorgan AI Research Introduces DocLLM: A Lightweight Extension to Traditional Large Language Models for Generative Reasoning Over Documents with Rich Layouts
Summary: JPMorgan AI Research presents DocLLM, an extension of large language models designed for reasoning over visual documents. It combines textual semantics and spatial layout using bounding box coordinates, allowing efficient cross-modal interaction capture. The model is pre-trained with a modified self-supervised target addressing layout issues and fine-tuned with instruction data for tasks like form comprehension, table alignment, and visual question answering, showing significant performance gains.

Link: https://www.marktechpost.com/2024/01/05/jpmorgan-ai-research-introduces-docllm-a-lightweight-extension-to-traditional-large-language-models-tailored-for-generative-reasoning-over-documents-with-rich-layouts/?amp=

<img src="/img/e3156297-1b6f-4fe0-b942-2483d95d261c.png" width="400" />


<sup><sub>1/6/2024 [Mark as read](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9805_0&tag=isread) [Mark as BestOf](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9805_0&tag=bestof) [Experiments](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9805_0&tag=Experiments)<sub/><sup/>

<br/><br/>

## JPMorgan AI Research Introduces DocLLM, a Lightweight Extension to Traditional Large Language Models for Generative Reasoning Over Documents with Rich Layouts
Summary: JPMorgan AI Research has introduced DocLLM, a lightweight extension to traditional Large Language Models (LLMs) specifically tailored for generative reasoning over documents with rich layouts. DocLLM represents both text semantics and spatial layouts, leveraging bounding box coordinates acquired through optical character recognition (OCR) to add spatial layout information. It extends the self-attention mechanism of transformers to capture cross-modal interactions between text and layout. DocLLM has demonstrated significant performance gains on various document intelligence tasks, including form comprehension, table alignment, visual question answering, and key information extraction, highlighting its effectiveness in handling complex document structures and mixed data types.

Link: https://www.marktechpost.com/2024/01/05/jpmorgan-ai-research-introduces-docllm-a-lightweight-extension-to-traditional-large-language-models-tailored-for-generative-reasoning-over-documents-with-rich-layouts/?amp=

<img src="/img/c5ac700d-366a-404f-a3e3-51dbeaa6f74e.png" width="400" />


<sup><sub>1/6/2024 [Mark as read](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9805_0&tag=isread) [Mark as BestOf](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9805_0&tag=bestof) [Experiments](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9805_0&tag=Experiments)<sub/><sup/>

<br/><br/>

## Curiosity, effort, ability, and luck are key factors in achieving great work. One should focus on something exciting that provides scope for great and unique work. It is recommended to choose a field, learn enough to reach the frontier of knowledge, notice gaps, and then explore promising ones.
Summary: This text describes the common traits of exceptionally productive people. The author asserts that the first step to excelling in a field is to choose a field that aligns with your natural aptitudes, interests, and offers ample opportunities for groundbreaking work. However, finding such a field is difficult, especially when you're young. Therefore, the author recommends experimenting with different fields and projects until you find one that truly excites you.

Once you've chosen a field, the next step is to learn as much as you can about it and identify knowledge gaps. Then, focus on filling those gaps by conducting research, asking questions, and seeking out experts in the field. The author emphasizes the importance of embracing strange or unconventional ideas, as these often lead to breakthroughs.

The author also stresses the significance of perseverance and hard work. Great work, they argue, often requires long hours and intense focus. However, it is important to avoid burnout by taking breaks and engaging in activities that recharge your energy.

To ensure consistency in your work, the author suggests setting clear goals and creating a schedule that allows for uninterrupted periods of focused work. Additionally, they recommend avoiding distractions and interruptions, both during work and during breaks.

To improve your work further, the author encourages seeking feedback from others, especially those who are knowledgeable in your field. Constructive criticism can help you identify areas where you can improve and refine your work.

Finally, the author highlights the importance of maintaining a curious and open mindset. By continuously seeking new knowledge and experiences, you can expand your understanding of the world and generate innovative ideas.

Link: https://paulgraham.com/greatwork.html

<img src="/img/46e14cdf-da78-48e6-90bb-10c9a4483a03.png" width="400" />


<sup><sub>12/28/2023 [Mark as read](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9657_0&tag=isread) [Mark as BestOf](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9657_0&tag=bestof) [Experiments](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=9657_0&tag=Experiments)<sub/><sup/>

<br/><br/>

## A List of Open-Source LLMs for Builders
Summary: This article presents a comprehensive list of open-source Large Language Models (LLMs) available for both commercial and research purposes. It includes models like Flan-U\u00b2, OpenChatKit, Cerebras-GPT, Pythia, Bloom & mTO, OpenAssistant, nanoT5, GeoV, Baize, Vicuna, Koala, GPT4All, Lit-LLaMA, Dolly, Dalai, Alpaca.cpp, Alpaca-LORA, and llama.cpp. While most of these models can be used commercially, Lit-LLaMA, Dolly, and OpenAssistant's offerings are restricted to non-commercial use. The article emphasizes that users should carefully consider their intended use case when selecting a model, as many require instruction tuning to perform effectively.

Link: https://www.linkedin.com/feed/update/urn:li:activity:7049789761728770049?utm_source=share&utm_medium=member_android

<img src="/img/ccc7d6d4-ae0a-4d05-9334-9fe130ed51dd.png" width="400" />


<sup><sub>4/8/2023 [Mark as read](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=8221_0&tag=isread) [Mark as BestOf](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=8221_0&tag=bestof) [Experiments](https://githublistbuilder.azurewebsites.net/api/TagSetter?articleid=8221_0&tag=Experiments)<sub/><sup/>

<br/><br/>

