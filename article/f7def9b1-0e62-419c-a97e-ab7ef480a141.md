## Novel Method Boosts Text Embeddings With Large Language Models
Summary: A novel method is proposed for obtaining high-quality text embeddings using only synthetic data and less than 1k training steps, eliminating the need for complex training pipelines and reliance on manually collected datasets. This method leverages proprietary LLMs to generate diverse synthetic data for numerous text embedding tasks across various languages. By fine-tuning open-source decoder-only LLMs on synthetic data using standard contrastive loss, the method showcases strong performance on benchmarks and sets new state-of-the-art results when fine-tuned with a mixture of synthetic and labeled data.

Link: https://arxiv.org/abs/2401.00368

<img src="/img/f7def9b1-0e62-419c-a97e-ab7ef480a141.png" width="400" />
<br/><br/>
