## MiniGPT-v2 and MiniGPT-4: Enhanced Vision-Language Understanding with Advanced Large Language Models
Summary: MiniGPT-4 and MiniGPT-v2 are two variants of large language models fine-tuned for vision-language multi-task learning. Both models can generate captions, answer questions, and classify images based on contextual prompts and image inputs. MiniGPT-4 is based on LLAMA/Vicuna models, while MiniGPT-v2 is based on BLIP-2, and they have been used in various applications such as instruction-based image generation, patent figure captioning, dermatology diagnosis, and artistic vision-language understanding.

Link: https://github.com/Vision-CAIR/MiniGPT-4

<img src="/img/d50dba0e-e12a-4ddc-b26e-0c117c67c96b.png" width="400" />
<br/><br/>
