## MosaicML Launches 30B Model — Takes on LLaMA, Falcon and GPT
Summary: MosaicML, a startup founded by Naveen Rao, has launched its second open-source large language model (LLM) called MPT-30B, which claims to surpass OpenAI’s GPT-3 in quality despite having fewer parameters. The model is trained on longer sequences and uses a technique called "FlashAttention" for faster inference and training. MosaicML emphasizes the importance of open-source models for industries like healthcare and banking, where data needs to be handled securely behind a firewall. Developers can use MosaicML's platform through an API, customize and fine-tune models with their own data, or even pre-train custom models from scratch. The company believes that open-source LLMs are closing the gap with closed-source models and empowering enterprise developers.

Link: https://thenewstack.io/mosaicml-launches-30b-model-takes-on-llama-falcon-and-gpt/

<img src="/img/163d6980-138d-4b14-819e-b5ffb76b1435.png" width="400" />
<br/><br/>
