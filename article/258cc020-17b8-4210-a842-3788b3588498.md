## MosaicML releases 30B OpenAI GPT-3-like model, MPT-30B, under Apache 2.0 license
Summary: MosaicML released their open-source MPT-30B model under Apache 2.0 license. The model, trained on 1 trillion tokens, has 30 billion parameters, supports partial training on H100, and utilizes techniques like FlashAttention, ALiBi, QK LayerNorm, and more. It has a chat version available on Hugging Face Space and can be found on Hugging Face for download.

Link: https://www.linkedin.com/posts/philipp-schmid-a6a2bb196_new-open-source-model-alertmosaicml-just-activity-7077671783960584192-9HDD?utm_source=share&amp;utm_medium=member_android

<img src="/img/258cc020-17b8-4210-a842-3788b3588498.png" width="400" />
<br/><br/>
