## TinyLlama: Open-Source Small Language Model Pretrained on 3 Trillion Tokens
Summary: The TinyLlama project aims to pretrain a small language model with 1.1B parameters on a massive dataset of 3 trillion tokens. With efficient training techniques and the use of powerful GPUs, the project aims to complete the training within approximately 90 days. The resulting model is expected to be compact and computationally efficient, enabling applications with restricted memory and computational resources. The project's code and trained models are open-sourced, inviting community contributions and feedback.

Link: https://github.com/jzhang38/TinyLlama

<img src="/img/50a7f90c-5e34-4df7-953e-37128f9380f4.png" width="400" />
<br/><br/>
