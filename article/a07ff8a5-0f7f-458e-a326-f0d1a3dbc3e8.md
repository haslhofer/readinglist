## MosaicML, a platform for training machine learning models, has acquired MosaicBERT, an efficient and cost-effective model for pretraining the BERT language model. With MosaicBERT, users can pretrain a competitive BERT-Base model from scratch for just $20, making it accessible to a wider range of researchers and engineers.
Summary: MosaicML announces a new optimized MosaicBERT architecture and training recipe that enables users to pretrain a high-quality BERT model from scratch on their own data for only \$20. This breakthrough makes it more accessible for researchers and engineers to pretrain custom BERT models for specific domains, leading to better models and competitive advantages. The MosaicBERT architecture incorporates architectural choices from recent transformer literature, including FlashAttention, ALiBi, unpadding, low precision LayerNorm, and Gated Linear Units, resulting in improved accuracy and faster training times compared to the standard BERT-Base. MosaicBERT also introduces training optimizations such as the MosaicML StreamingDataset, higher masking ratio for the Masked Language Modeling objective, bfloat16 precision, and increased vocab size, further enhancing efficiency and quality. The finetuning performance of MosaicBERT-Base surpasses the baseline BERT-Base on four out of eight GLUE tasks and achieves comparable performance on the rest. Additionally, MosaicBERT-Large demonstrates a 1.47x speedup over the baseline BERT-Large. Overall, MosaicBERT empowers researchers and engineers to build better models for their specific domains without time and cost constraints.

Link: https://www.mosaicml.com/blog/mosaicbert

<img src="/img/a07ff8a5-0f7f-458e-a326-f0d1a3dbc3e8.png" width="400" />
<br/><br/>
