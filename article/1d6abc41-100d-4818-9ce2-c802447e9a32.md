## MosaicML joins forces with Databricks to accelerate data-driven AI
Summary: MosaicML, a leading provider of AI training and deployment tools, has released MPT-30B, a new open-source foundation model with significantly improved performance compared to its predecessor, MPT-7B. MPT-30B is specifically designed for commercial use and outperforms the original GPT-3 in various tasks. It also includes two fine-tuned variants: MPT-30B-Instruct for single-turn instruction following and MPT-30B-Chat for multi-turn conversations. MosaicML offers customization and deployment options for MPT-30B through its platform, including fine-tuning, domain-specific pre-training, and training from scratch. Additionally, the company has open-sourced its production-ready training code as LLM Foundry, enabling users to train custom language models efficiently. MPT-30B offers advantages such as a large context window of 8k tokens, efficient inference and training performance, and strong coding abilities. For evaluation, MosaicML presents a fast in-context learning evaluation framework that significantly outperforms other harnesses in terms of speed.

Link: https://www.mosaicml.com/blog/mpt-30b

<img src="/img/1d6abc41-100d-4818-9ce2-c802447e9a32.png" width="400" />
<br/><br/>
