## Research study concludes large language models learn almost all knowledge during pretraining requiring minimal instruction tuning
Summary: A novel language model called LIMA, trained with just 1,000 curated prompts and responses without reinforcement learning or human preference modeling, has demonstrated strong performance, comparable to GPT-4, Bard, and DaVinci003, suggesting that most knowledge in large language models is acquired during pretraining.

Link: https://arxiv.org/abs/2305.11206

<img src="/img/4dc2a12b-1de4-493f-bdd8-73ce2ffe7a6b.png" width="400" />
<br/><br/>
