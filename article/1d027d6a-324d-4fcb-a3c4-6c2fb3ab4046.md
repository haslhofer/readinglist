## Layout-Aware Generative Language Model for Understanding Visual Documents
Summary: DocLLM is a lightweight extension to traditional large language models (LLMs) for understanding visual documents, combining textual semantics and spatial layout. It utilizes bounding box information and a disentangled attention mechanism to capture the relationship between text and spatial modalities. The pre-trained model is fine-tuned on a large-scale instruction dataset, excelling on 14 out of 16 document intelligence tasks across various datasets and generalizing well to new unseen datasets.

Link: https://arxiv.org/abs/2401.00908

<img src="/img/1d027d6a-324d-4fcb-a3c4-6c2fb3ab4046.png" width="400" />
<br/><br/>
