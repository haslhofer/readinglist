## Parameter-Efficient Finetuning of Large Language Models Using Adapters
Summary: Finetuning large language models (LLMs) can be expensive and time-consuming, so researchers have developed parameter-efficient methods like adapters. Adapters are tunable layers added to transformer blocks of an LLM, enabling fine-tuning with a small number of new parameters. In this approach, a single adapter layer is inserted in two places within each transformer block. The first fully connected layer projects the input down to a low-dimensional representation, and the second fully connected layer projects it back into the input dimension. Adapters allow for fine-tuning with a much smaller number of parameters compared to traditional methods while achieving comparable performance.

Link: https://magazine.sebastianraschka.com/p/finetuning-llms-with-adapters

<img src="/img/c9bbde68-874f-411d-b99d-2c3ed094d7a4.png" width="400" />
<br/><br/>
