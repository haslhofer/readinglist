## Long-context embedding models can enhance retrieval in RAG by grounding it in high-level semantic context
Summary: Long-context embedding models have the potential to alleviate the "embedding chunking" problem in RAG by grounding retrieval in higher-level semantic context. One way to include long-context embeddings is through "hybrid" retrieval, which combines standard similarity search with long-context embedding-based document similarity, allowing for more comprehensive retrieval of relevant documents.

Link: https://www.linkedin.com/posts/llamaindex_a-cool-promise-for-long-context-embedding-activity-7154645081163993088-dAKB?utm_source=share&amp;utm_medium=member_android

<img src="/img/9e275965-12d8-4a49-93af-b2b6dcb07c8e.png" width="400" />
<br/><br/>
