## Yarn-Llama-2-13b-128k: A Model Trained for Extreme Context Length Released by Enrico Shippole
Summary: Enrico Shippole, an ML Engineer, announced the release of a new Yarn-Llama-2-13b-128k model, trained for 128k context length using YaRN scaling. Collaborators Bowen, Jeff, and Honglu contributed to the model's development. The model can be found on HuggingFace and surpasses the performance of previous methodologies in all evaluations while maintaining perplexity at 128k extrapolation. Additionally, Yarn-Llama-2-7b models trained for 128k and 64k context lengths are also available. The team open-sourced all the code for full reproducibility of the results. The models perform similarly to the base LLaMA 2 models on Open LLM benchmarks while directly scaling context length to 128k. The compute resources were generously sponsored by CarperAI, Emad Mostaque, and Stability AI. The team appreciates the support of Dakota Mahan, Jon tow, and Honglu for their contributions.

Link: https://www.linkedin.com/feed/update/urn:li:activity:7103092498536824832?utm_source=share&amp;utm_medium=member_android

<img src="/img/b1ae9de4-ad7b-462d-9d58-2ce5a1825b43.png" width="400" />
<br/><br/>
