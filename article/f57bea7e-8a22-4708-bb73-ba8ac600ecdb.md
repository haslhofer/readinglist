## Kosmos-1: A Multimodal Large Language Model that Can See, Reason, and Act
Summary: Researchers introduced Kosmos-1, a multimodal large language model (MLLM) capable of perceiving general modalities, learning in context, and following instructions. Trained from scratch on web-scale multimodal corpora, Kosmos-1 demonstrated impressive performance on various tasks spanning language understanding, generation, OCR-free NLP, perception-language tasks, and vision tasks. Experiments revealed the benefits of cross-modal transfer between language and multimodal modalities. Furthermore, a new dataset was introduced to assess the nonverbal reasoning capabilities of MLLMs.

Link: https://arxiv.org/abs/2302.14045

<img src="/img/f57bea7e-8a22-4708-bb73-ba8ac600ecdb.png" width="400" />
<br/><br/>
