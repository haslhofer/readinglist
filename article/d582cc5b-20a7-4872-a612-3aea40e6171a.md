## LLaMA Adapter: Fine-tuning Large Language Models with Minimal Parameters and Computational Cost
Summary: LLaMA-Adapter is a lightweight and efficient method for fine-tuning large language models like LLaMA for instruction-following tasks. It uses a small set of learnable adaption prompts and a zero-initialized attention mechanism to inject instructional cues into the model without fully fine-tuning the entire model, resulting in fast adaptation with comparable performance to fully fine-tuned models. The approach can also be extended to multi-modal instructions for learning image-conditioned language models and can be used to fine-tune other pre-trained models for various tasks, demonstrating its generalization capability.

Link: https://paperswithcode.com/paper/llama-adapter-efficient-fine-tuning-of

<img src="/img/d582cc5b-20a7-4872-a612-3aea40e6171a.png" width="400" />
<br/><br/>
