## A 32k long pretrained Togethercomputer's M2-BERT model that generates embeddings for retrieval tasks
Summary: Monarch Mixer-BERT is a pre-trained transformer model designed for long-context retrieval. Intended to generate embeddings for retrieval, it was trained with a sequence length of 32768 and has a dimensionality of 768. Its architecture is based on M2-BERT, a masked sequence-to-sequence model. This model has been fine-tuned for long-context retrieval, exhibiting strong performance in extracting meaningful embeddings from long text.

Link: https://huggingface.co/togethercomputer/m2-bert-80M-32k-retrieval

<img src="/img/4aca320a-25ce-4a60-b73e-0a5b3c39c3e2.png" width="400" />
<br/><br/>
