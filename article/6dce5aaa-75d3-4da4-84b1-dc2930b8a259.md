## New Multimodal Large Language Model 'Ferret' Accurately Grounds Open-Vocabulary Descriptions to Any Image Region
Summary: Ferret is a new Multimodal Large Language Model (MLLM) that can understand spatial referring and accurately ground open-vocabulary descriptions within an image. It employs a novel hybrid region representation that integrates discrete coordinates and continuous features to represent a region in the image and uses a spatial-aware visual sampler to extract continuous features of regions with varying shapes and sizes. Ferret is evaluated on a comprehensive dataset called GRIT, which contains 1.1M samples with 95K hard negative data, and achieves superior performance in classical referring and grounding tasks, as well as region-based and localization-demanded multimodal chatting, demonstrating improved capability in describing image details and reducing object hallucination.

Link: https://arxiv.org/abs/2310.07704v1

<img src="/img/6dce5aaa-75d3-4da4-84b1-dc2930b8a259.png" width="400" />
<br/><br/>
