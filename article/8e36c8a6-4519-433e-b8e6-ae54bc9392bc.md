## Smaller Models Outperform LLMs with 2000X Fewer Parameters
Summary: A new method called "Distilling Step-by-Step" trains smaller models that outperform larger language models (LLMs) using a fraction of the data. The method uses Chain of Thought (CoT) prompting to ask the LLM to generate steps of logical thinking, which are then used to train the smaller models. The smaller models outperform LLMs on various tasks, even with unlabeled data. This research demonstrates the potential for training smaller, more efficient models that can perform complex tasks with limited data.

Link: https://www.linkedin.com/posts/sanyambhutani_outperforming-llms-with-2000x-smaller-models-activity-7060977553104134144-1gRH?utm_source=share&amp;utm_medium=member_android

<img src="/img/8e36c8a6-4519-433e-b8e6-ae54bc9392bc.png" width="400" />
<br/><br/>
