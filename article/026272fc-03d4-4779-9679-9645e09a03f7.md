## Multimodal agents that interact with smartphone apps like humans
Summary: Researchers have introduced a new multimodal agent framework called AppAgent, which utilizes large language models (LLMs) to operate smartphone applications through a simplified action space. This novel approach enables the agent to interact with apps like a human user, tapping and swiping to navigate and perform tasks. The agent's learning process involves autonomous exploration or observing human demonstrations to generate a knowledge base for executing complex tasks across different apps. Extensive testing over 50 tasks in 10 diverse applications demonstrated the agent's proficiency in handling a wide variety of high-level tasks.

Link: https://arxiv.org/abs/2312.13771

<img src="/img/026272fc-03d4-4779-9679-9645e09a03f7.png" width="400" />
<br/><br/>
