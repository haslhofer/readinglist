## Researchers Introduce Kosmos-1: A Multimodal Large Language Model for Perception and Language Alignment
Summary: Kosmos-1, a Multimodal Large Language Model (MLLM), is introduced that can perceive general modalities, learn in context (few-shot), and follow instructions (zero-shot). It is trained from scratch on web-scale multimodal corpora, including text and images, image-caption pairs, and text data. Kosmos-1 achieves impressive performance on language understanding, generation, OCR-free NLP, perception-language tasks, and vision tasks. Additionally, MLLMs can benefit from cross-modal transfer, and a dataset of Raven IQ test is introduced to diagnose the nonverbal reasoning capability of MLLMs.

Link: https://arxiv.org/abs/2302.14045

<img src="/img/089a2445-04a6-490f-8c2b-ba7877fd1b39.png" width="400" />
<br/><br/>
