## Efficient Large Language Model Inference with Limited Memory
Summary: Researchers have developed an efficient method for running large language models (LLMs) on devices with limited DRAM capacity by storing model parameters on flash memory and bringing them on demand to DRAM. Two techniques, "windowing" and "row-column bundling", strategically reduce data transfer from flash memory and increase the size of data chunks read, respectively. These methods allow models up to twice the size of the available DRAM to be run with a significant increase in inference speed compared to traditional approaches. This combination of sparsity awareness, context-adaptive loading, and a hardware-oriented design enables LLMs to be used effectively on devices with limited memory.

Link: https://arxiv.org/abs/2312.11514

<img src="/img/24981d43-c5df-444f-a7e3-14575d69abaf.png" width="400" />
<br/><br/>
