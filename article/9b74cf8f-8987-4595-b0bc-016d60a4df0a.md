## LLongMA-2 13b is released, a Llama-2 model trained at 8k context length using linear positional interpolation scaling.
Summary: Enrico Shippole, an ML Engineer, released LLongMA-2 13b, a Llama-2 model trained at an 8k context length using linear positional interpolation scaling. This model was developed in collaboration with Jeff of NousResearch and Kaiokendev. The model can be found on Hugging Face and has surpassed other methodologies in evaluations, maintaining perplexity at 8k extrapolation. A Llama-2 7b model trained at 16k context length will also be released on Hugging Face. The model works out-of-the-box with the new version of transformers (4.31) or with `trust_remote_code` for earlier versions. The method's application to rotary position embedding only requires minor changes to the model's code. The repository containing Jeffâ€™s implementation of scaled rotary embeddings is provided. The model was further trained on Together Compute's Red Pajama dataset, and the pre-tokenized dataset will be made available soon. Enrico recommends Ofir Press's research on ALiBi, which laid the foundation for many of these scaling techniques. He also suggests reviewing the paper, A Length-Extrapolatable Transformer, and xPos technique, which also apply scaling to rotary embeddings. Enrico previously trained the first publicly available model with rotary embedding scaling. This model release was sponsored by CarperAI, Emad Mostaque, and Stability AI. It is not an official Stability AI product.

Link: https://www.linkedin.com/feed/update/urn:li:activity:7089288709220524032?utm_source=share&amp;utm_medium=member_android

<img src="/img/9b74cf8f-8987-4595-b0bc-016d60a4df0a.png" width="400" />
<br/><br/>
