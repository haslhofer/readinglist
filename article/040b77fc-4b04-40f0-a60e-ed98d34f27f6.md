## LMSYS successfully extended the context length of Meta's LLaMA model from 2048 to 16384 tokens through rotary position embedding condensation.
Summary: Researchers successfully extended the context length of Meta's LLaMA model from 2048 to 16384 tokens by condensing the Rotary position embedding as suggested by Kaiokendev. The evaluation toolkit and the chat model are impressive additions, demonstrating the power of open-source and open science in driving innovation.

Link: https://www.linkedin.com/posts/philipp-schmid-a6a2bb196_%3F%3F%3F%3F%3F%3F-%3F%3F-%3F%3F%3F%3F%3F%3F%3F%3F%3F-%3F%3F-activity-7080432121059667968-skA3?utm_source=share&amp;utm_medium=member_desktop

<img src="/img/040b77fc-4b04-40f0-a60e-ed98d34f27f6.png" width="400" />
<br/><br/>
