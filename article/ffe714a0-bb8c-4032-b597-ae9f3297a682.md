## Sure, here is a one-line headline describing the provided text:

**MoEs: Efficiently pretraining and serving large language models with mixture-of-experts.**
Summary: Mixture of Experts (MoE) is a type of transformer model that uses sparsity to enable faster pretraining and inference compared to dense models. MoEs consist of sparse MoE layers, which have a certain number of "experts" (e.g. 8), where each expert is a neural network. A gate network or router determines which tokens are sent to which expert. MoEs have been used to train multi-trillion parameter models, such as the open-sourced 1.6T parameters Switch Transformers. Fine-tuning MoEs has historically been difficult due to overfitting, but recent work with MoE instruction-tuning has shown promise.

Link: https://huggingface.co/blog/moe

<img src="/img/ffe714a0-bb8c-4032-b597-ae9f3297a682.png" width="400" />
<br/><br/>
