## Introducing LLaMA-Adapter, zero-init attention for fine-tuning large language models.
Summary: LLaMA-Adapter introduces an efficient adaptation method for fine-tuning large language models like LLaMA for instruction-following tasks. Using a set of learnable adaptation prompts, it adaptively integrates instructional cues into the model while preserving its pre-trained knowledge. Demonstrating strong performance on tasks like language commands and multi-modal instructions, it outperforms other approaches with significantly fewer parameters and training time.

Link: https://paperswithcode.com/paper/llama-adapter-efficient-fine-tuning-of

<img src="/img/55221f0c-3e9c-4763-934d-326963ff2579.png" width="400" />
<br/><br/>
