## MosaicML Introduces MPT-30B, a 30B Parameter LLM, Competing with LLaMA and Falcon in the Open Source Arena
Summary: MosaicML has launched its second big language model (LLM), called MPT-30B, which follows on from the smaller MPT-7B model it debuted in May. The new model is designed to handle even longer sequences in practice, making it a perfect fit for data-heavy enterprise applications. MosaicML claims that its new 30B parameter model also compares favorably to both LLaMA and Falcon in performance.

Link: https://thenewstack.io/mosaicml-launches-30b-model-takes-on-llama-falcon-and-gpt/

<img src="/img/8417ba5b-acaf-43be-b83e-6f4e128f0fe3.png" width="400" />
<br/><br/>
