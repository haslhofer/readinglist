## Recurrent Memory Augmentation Enables GPT-2 to Process 11M Token Sequences, a Record for Neural Networks
Summary: Researchers have developed a new benchmark, BABILong, to evaluate the capabilities of generative transformer models in extracting and processing distributed facts within extensive texts. They found that common methods are effective only for sequences up to 10^4 elements, but fine-tuning GPT-2 with recurrent memory augmentations enabled it to handle tasks involving up to 11x10^6 elements, representing a significant improvement in the processing capabilities for long sequences.

Link: https://arxiv.org/abs/2402.10790

<img src="/img/a328c53a-fffa-43df-a33d-7ea5496b10c6.png" width="400" />
<br/><br/>
