## MosaicML releases MPT-30B, a 30B parameter open-source model, trained on 1 trillion tokens.
Summary: MosaicML released MPT-30B, a 30B parameter open-source model trained on 1 trillion tokens under the Apache 2.0 license. The model is available on Hugging Face and can be used for various natural language processing tasks. It was trained with techniques like FlashAttention, ALiBi, and QK LayerNorm. A chat version of the model is also available as a Hugging Face Space.

Link: https://www.linkedin.com/posts/philipp-schmid-a6a2bb196_new-open-source-model-alertmosaicml-just-activity-7077671783960584192-9HDD?utm_source=share&amp;utm_medium=member_android

<img src="/img/dea07def-e71d-4c44-a89e-e5474dfd66e3.png" width="400" />
<br/><br/>
