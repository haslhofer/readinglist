## New technique enables inference of 70B LLM on a single 4GB GPU
Summary: The article discusses various techniques for optimizing memory usage during inference with large language models (LLMs) like the 70B LLM, enabling inference on a single 4GB GPU. These techniques include layer-wise inference, flash attention, model file sharding, the use of a meta device, and an open-source library called AirLLM. These methods allow for significant memory savings without sacrificing model performance, making it possible to run inference with large models on limited hardware. While training large models on a single GPU is not feasible due to memory constraints, gradient checkpointing is mentioned as a potential technique for reducing training memory requirements.

Link: https://ai.gopubby.com/unbelievable-run-70b-llm-inference-on-a-single-4gb-gpu-with-this-new-technique-93e2057c7eeb

<img src="/img/78a6c823-a695-4270-b826-8313403d7d9c.png" width="400" />
<br/><br/>
