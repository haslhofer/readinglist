## Smaller LLMs Outshine Larger Models: Efficiency and Capability Redefined in AI
Summary: In 2023, smaller LLMs (language models with 13 billion parameters or less) have been gaining attention for their impressive performance despite their size. These models, such as DeciCoder-1B, Phi-1.5, Dolly-v2-3b, StableLM-Zephyr-3B, DeciLM-7B, Mistral-7B-Instruct-v0.2, Amber, OpenHathi-7B-Hi-v0.1-Base, SOLAR-10.7B-v1.0, and NexusRaven-V2-13B, have shown remarkable capabilities in various NLP tasks, challenging the notion that only large models can produce excellent results. These smaller LLMs offer efficiency, versatility, and affordability, making them valuable tools for research, industry, and society as a whole. Their emergence has transformed the LLM landscape and opened up new possibilities for advancing natural language understanding and generation.

Link: https://deci.ai/blog/small-giants-top-10-under-13b-llms-in-open-source/

<img src="/img/da210f59-0d8f-4c5b-8be3-cae5239d67bb.png" width="400" />
<br/><br/>
