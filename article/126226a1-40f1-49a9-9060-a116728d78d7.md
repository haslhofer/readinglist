## LLMs, especially GPT-4, are not as reliable as humans in evaluating large language models (LLMs) due to positional bias and preference for GPT-4 trained data.
Summary: This article compares the effectiveness of Large Language Models (LLMs) to human labelers in evaluating instruction-tuned models. A preference dataset was generated by soliciting human evaluations on a diverse set of prompts, then using these labels to train an Elo-based preference model. GPT-4 was then used to generate evaluations on the same prompts. Results show that ratings from GPT-4 and human annotators have a moderate correlation, and that GPT-4 is predisposed to prefer models trained on data bootstrapped using InstructGPT/GPT-4/ChatGPT over more factual and useful content. The study also found that GPT-4 has a positional bias, preferring models that are presented first in the prompt. Overall, it concludes that, while LLMs can be useful for evaluating certain types of tasks, they are not yet a reliable replacement for human labelers.

Link: https://huggingface.co/blog/llm-leaderboard

<img src="/img/126226a1-40f1-49a9-9060-a116728d78d7.png" width="400" />
<br/><br/>
