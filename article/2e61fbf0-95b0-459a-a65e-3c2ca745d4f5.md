## GPU Optimizations Speed Large Diffusion Models for On-Device Use
Summary: The research paper titled "Speed Is All You Need: On-Device Acceleration of Large Diffusion Models via GPU-Aware Optimizations" introduces a series of optimizations that enable large diffusion models to run on GPU-equipped mobile devices with unprecedented speed. The optimized models achieve an inference latency of under 12 seconds for Stable Diffusion 1.4 on the Samsung S23 Ultra, making them the fastest reported on-device implementation to date. This breakthrough expands the practical applications of generative AI and significantly enhances the user experience across a broad range of devices.

Link: https://arxiv.org/abs/2304.11267

<img src="/img/2e61fbf0-95b0-459a-a65e-3c2ca745d4f5.png" width="400" />
<br/><br/>
