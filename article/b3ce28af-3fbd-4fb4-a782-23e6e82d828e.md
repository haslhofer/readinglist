## Direct Preference Optimization Technique Improves Performance of Supervised Fine-Tuned Models
Summary: In this article, the author introduces Direct Preference Optimization (DPO), a fine-tuning technique for large language models (LLMs) that uses preference datasets to align the model's outputs with human preferences. The author demonstrates the use of DPO to fine-tune the OpenHermes-2.5 model using the Intel/orca_dpo_pairs dataset. The resulting model, NeuralHermes-2.5, shows significant improvement in performance over the Open LLMs Leaderboard. The author further discusses potential areas for improvement and provides references for further reading.

Link: https://towardsdatascience.com/fine-tune-a-mistral-7b-model-with-direct-preference-optimization-708042745aac

<img src="/img/b3ce28af-3fbd-4fb4-a782-23e6e82d828e.png" width="400" />
<br/><br/>
