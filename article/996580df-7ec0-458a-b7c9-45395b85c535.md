## TinyLlama: Aiming to Pretrain a 1.1B Llama Model on 3 Trillion Tokens
Summary: The TinyLlama project aims to pretrain a 1.1B Llama model on 3 trillion tokens within 90 days using 16 A100-40G GPUs. The training started on September 1, 2023, and the project is expected to be completed by December 28, 2023. The model is compact with only 1.1B parameters, allowing it to cater to applications demanding restricted computation and memory footprint. Intermediate checkpoints will be released following a schedule, and the project is open to community feedback and contributions.

Link: https://github.com/jzhang38/TinyLlama

<img src="/img/996580df-7ec0-458a-b7c9-45395b85c535.png" width="400" />
<br/><br/>
