## Memory-efficient pre-training of 7B models on consumer-grade GPUs
Summary: Researchers have developed GaLore, a method to train large language models (LLMs) on consumer-grade GPUs with limited memory. GaLore projects gradients to lower ranks during training, allowing models with up to 7B parameters to be fully trained on a single 24Gb GPU. The method combines gradient low-rank projection with other efficiency techniques like 8-bit Adam, allowing for significant memory reduction without performance loss. This democratizes access to LLM training by enabling researchers and developers to utilize consumer-grade GPUs for training, fostering a more inclusive field and expanding AI capabilities.

Link: https://www.linkedin.com/posts/a-roucher_%3F%3F%3F%3F%3F%3F-%3F%3F%3F%3F%3F-%3F%3F-%3F%3F%3F%3F%3F%3F-activity-7173730493232750595-ivM2?utm_source=share&amp;utm_medium=member_android

<img src="/img/79c286ce-f5a4-4a69-a63e-639964c3f8e1.png" width="400" />
<br/><br/>
