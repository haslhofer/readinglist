## Multi-Modal in LlamaIndex
Summary: Large Multi-modal Models (LMMs) are a generalization of Large Language Models, allowing for the joint input of both images and text, and output text. This enables a wide range of applications, including image reasoning, image understanding, and retrieval augmented generation. LlamaIndex provides various features to support the development of Multi-Modal RAGs, such as Multi-Modal LLM support, Multi-Modal vector stores, and a Simple Multi-Modal Query Engine. The documentation provides comprehensive usage patterns, code snippets, and examples to help users build their own Multi-Modal RAG pipelines. Additionally, it includes a table summarizing the compatibility of different Multi-Modal LLM models and Multi-Modal vector stores, as well as links to tutorials and example notebooks.

Link: https://docs.llamaindex.ai/en/latest/module_guides/models/multi_modal.html#multi-modal-llm-models

<img src="/img/a98883c6-e7ce-40da-b72d-ee5de0dc6ed7.png" width="400" />
<br/><br/>
