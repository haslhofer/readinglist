## MosaicML, a cloud-based machine learning (ML) platform provider, announced that it has been acquired by Databricks, the popular data and AI platform. The acquisition aims to enhance Databricks' ML offerings by integrating MosaicML's expertise in building and training large language models (LLMs) efficiently. Customers can now leverage the combined capabilities of both companies to accelerate their AI initiatives.
Summary: With the MosaicBERT architecture and training recipe, researchers can pretrain a BERT-Base model from scratch on the MosaicML platform for just $20, matching the original BERT's average GLUE score of 79.6 in just 1.13 hours on 8 A100 GPUs. This enables researchers and engineers to pretrain custom BERT models on their own domain-specific data without time and cost constraints. Benchmarking against Hugging Face's BERT-Base, MosaicBERT-Base consistently achieved higher accuracy more quickly across all training durations. The MosaicBERT architecture incorporated architectural choices from recent transformer literature, such as FlashAttention, ALiBi, unpadding, low precision LayerNorm, and Gated Linear Units. Pretraining optimizations included using the MosaicML StreamingDataset, a higher masking ratio for the Masked Language Modeling objective, bfloat16 precision, and setting the vocab size to be a multiple of 8 and 64.

Link: https://www.mosaicml.com/blog/mosaicbert

<img src="/img/b4fe70ec-6332-4bb2-93d2-1dfabae7756d.png" width="400" />
<br/><br/>
