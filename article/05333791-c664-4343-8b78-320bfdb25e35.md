## LLaMA-Adapter: Fine-tuning Language Models with Zero-init Attention
Summary: LLaMA-Adapter is a codebase developed for efficient fine-tuning of LLaMA, a large language model. It demonstrates fine-tuning LLaMA to follow instructions within 1 hour and with only 1.2M parameters, making it well-suited for applications where fine-tuning time and computational resources are limited.

Link: https://github.com/ZrrSkywalker/LLaMA-Adapter

<img src="/img/05333791-c664-4343-8b78-320bfdb25e35.png" width="400" />
<br/><br/>
