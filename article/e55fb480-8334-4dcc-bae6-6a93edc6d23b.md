## Hugging Face Introduces TinyLlama, A Compact 1.1B Parameter Chat Model
Summary: The TinyLlama project aims to pretrain a 1.1B Llama model on 3 trillion tokens in 90 days using 16 A100-40G GPUs. It uses the same architecture and tokenizer as Llama 2 and is optimized for applications with restricted computation and memory footprint. The model is fine-tuned on the UltraChat and UltraFeedback datasets and can be used for text generation tasks. It can be loaded on the Inference API on-demand.

Link: https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0

<img src="/img/e55fb480-8334-4dcc-bae6-6a93edc6d23b.png" width="400" />
<br/><br/>
