## Hugging Face's Whisper Fine-tuning Gets 5x Faster without Performance Loss
Summary: Hugging Face has developed a new method for fine-tuning Whisper, a large-scale speech recognition model, which is 5 times faster than the previous method. This improvement allows for larger batch sizes, enabling the fine-tuning of Whisper-large checkpoints with less than 8GB of VRAM, resulting in minimal degradation in WER (Word Error Rate). The new method, powered by LoRA (Low-rank Adaptation) and PEFT (Personalized Fine-tuning), enables efficient fine-tuning of Whisper on custom datasets.

Link: https://www.linkedin.com/feed/update/urn:li:activity:7064229705507332096?utm_source=share&amp;utm_medium=member_android

<img src="/img/7fe823bf-0f19-4be1-9fbe-201ae18adf0a.png" width="400" />
<br/><br/>
