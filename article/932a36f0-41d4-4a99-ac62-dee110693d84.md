## LLaMA-Adapter: A Lightweight Approach for Fine-tuning LLaMA with Zero-init Attention
Summary: LLaMA-Adapter is a lightweight method that efficiently fine-tunes LLaMA, a large language model, for instruction-following tasks. It introduces a small number of learnable parameters and uses a zero-initialized attention mechanism to inject instructional cues into LLaMA while preserving its pre-trained knowledge. LLaMA-Adapter can generate high-quality responses comparable to models with fully fine-tuned parameters and can be extended to multi-modal instructions for learning image-conditioned LLaMA models.

Link: https://paperswithcode.com/paper/llama-adapter-efficient-fine-tuning-of

<img src="/img/932a36f0-41d4-4a99-ac62-dee110693d84.png" width="400" />
<br/><br/>
