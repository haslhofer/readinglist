## Fine-tuning LLMs with Amazon SageMaker Using Hugging Face and PyTorch FSDP
Summary: This article provides a detailed guide on fine-tuning a 20B-parameter language model (LLM) using PyTorch Fully Sharded Data Parallel (FSDP) with Amazon SageMaker and Hugging Face. The author demonstrates the process of setting up the environment, loading and preparing the dataset, and training the LLM on a multi-node multi-GPU setup. The guide covers topics such as installing required packages, creating a SageMaker session and IAM role, loading and preprocessing the dataset, implementing causal language modeling with FSDP, and launching the training job on Amazon SageMaker. The author highlights the benefits of using Amazon SageMaker and FSDP for training LLMs and provides cost estimates for running the training job on the ml.p4d.24xlarge instance.

Link: https://www.philschmid.de/sagemaker-fsdp-gpt

<img src="/img/6f0aaca3-7ad7-4c9b-8b68-dcd9ced9affe.png" width="400" />
<br/><br/>
