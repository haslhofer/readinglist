## Extension of Llama-2 model context length through fine-tuning with 128k context length using YaRN scaling
Summary: Enrico Shippole, an ML engineer, releases Yarn-Llama-2-13b-128k, a Llama-2 model trained for 128k context length using YaRN scaling. This model surpasses the performance of NTK-part scaling and maintains the same perplexity at 128k extrapolation. Yarn-Llama-2-7b and Yarn-Llama-2-13b models trained for 64k context length are also available. The code, open-source, is released for the reproduction of the paper's results. Collaborators, including Bowen and Jeff of NousResearch and Honglu of EleutherAI, are acknowledged. The compute for these models is sponsored by CarperAI, Emad Mostaque, and Stability AI.

Link: https://www.linkedin.com/feed/update/urn:li:activity:7103092498536824832?utm_source=share&amp;utm_medium=member_android

<img src="/img/3e9ca05c-bb06-43fa-9bd0-e3211f02773b.png" width="400" />
<br/><br/>
