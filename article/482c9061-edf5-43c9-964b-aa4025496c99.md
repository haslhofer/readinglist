## GPU-Aware Optimizations Accelerate Large Diffusion Models on Mobile Devices
Summary: Researchers have developed implementation optimizations for large diffusion models, achieving the fastest reported inference latency to-date on GPU-equipped mobile devices. These optimizations reduce the inference time of Stable Diffusion 1.4 to under 12 seconds on a Samsung S23 Ultra for a 512x512 image with 20 iterations, without using int8 quantization. This breakthrough enables on-device deployment of large diffusion models, broadening their applicability and improving the overall user experience across a wide range of devices.

Link: https://arxiv.org/abs/2304.11267

<img src="/img/482c9061-edf5-43c9-964b-aa4025496c99.png" width="400" />
<br/><br/>
