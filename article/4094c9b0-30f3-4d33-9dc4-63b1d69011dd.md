## MosaicML, a leading AI platform, has announced the integration of MosaicML into Databricks. MosaicML is known for its expertise in developing foundation models, which are trained on a large amount of diverse data and can be used for various tasks, including natural language processing, computer vision, and code generation.
Summary: MosaicML NLP team released MPT-7B, an open-source 7B-parameter transformer model that matches the quality of LLaMA-7B and outperforms other open-source models on academic tasks. MPT-7B was trained on 1 trillion tokens of text and code in 9.5 days with zero human intervention at a cost of ~$200k. The team also released three finetuned models: MPT-7B-StoryWriter-65k+, MPT-7B-Instruct, and MPT-7B-Chat, demonstrating the model's versatility for various tasks. The MPT model series is commercially usable, trained on a large dataset, can handle extremely long inputs, and is optimized for fast training and inference. The research team rigorously evaluated MPT on various benchmarks, and it met the high-quality standards set by LLaMA-7B. They hope businesses and the open-source community will build on this effort, leveraging the open-sourced codebase and tools for pretraining, finetuning, and evaluating MPT models.

Link: https://www.mosaicml.com/blog/mpt-7b

<img src="/img/4094c9b0-30f3-4d33-9dc4-63b1d69011dd.png" width="400" />
<br/><br/>
