## Finetuning large language mode
Summary: Finetuning large language models (LLMs) can be expensive in terms of computational resources and time. To address this, parameter-efficient methods have been developed, such as prompt and prefix tuning. This article focuses on the adapter method, which adds tunable layers to the transformer blocks of an LLM. Adapters are parameter-efficient as they only require training a small number of newly added parameters while freezing the parameters of the pretrained LLM. Experiments show that the adapter method can achieve comparable performance to fully finetuned LLMs while requiring significantly fewer parameters.

Link: https://magazine.sebastianraschka.com/p/finetuning-llms-with-adapters

<img src="/img/c4676fe7-db97-4fea-bfee-23e0389e4d7e.png" width="400" />
<br/><br/>
