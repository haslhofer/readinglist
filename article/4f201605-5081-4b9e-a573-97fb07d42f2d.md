## Cerebras releases pre-trained checkpoints of GPT-3 model with 13B parameters
Summary: Cerebras, a major technology company, has released a trained version of GPT-3 (from 111MB to 13B parameters) on the PILE Dataset, accelerated by Cerebras Wafer-Scale Clusters. This release is significant because it has the highest accuracy models for a compute budget and is available open-source with an Apache 2.0 license, allowing for royalty-free use for research or commercial applications.

Link: https://www.linkedin.com/posts/ebarsoum_cerebras-cerebras-activity-7046571544940064768-amSS?utm_source=share&amp;utm_medium=member_android

<img src="/img/4f201605-5081-4b9e-a573-97fb07d42f2d.png" width="400" />
<br/><br/>
