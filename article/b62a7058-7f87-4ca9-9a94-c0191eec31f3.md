## Fine-tuning a 20B+ LLM using AWS SageMaker with Hugging Face and PyTorch FSDP
Summary: This tutorial demonstrates how to fine-tune a large language model (LLM), GPT-NeoXT-Chat-Base-20B, using PyTorch Fully Sharded Data Parallel (FSDP) on Amazon SageMaker with Hugging Face Transformers. The setup involves processing the ELI5 dataset to create a "chat" version and chunking it into 2048 token segments. The training is facilitated by the HuggingFace Estimator in SageMaker, which handles the management of the distributed training environment. FSDP is employed to efficiently distribute the model and data across multiple nodes and GPUs. The training takes approximately 2.6 hours on 2x ml.p4d.24xlarge instances, resulting in a total cost of $197. The author highlights the benefits of using Amazon SageMaker and PyTorch FSDP for training LLMs and suggests further optimizations such as using spot instances or parameter-efficient fine-tuning to reduce costs.

Link: https://www.philschmid.de/sagemaker-fsdp-gpt

<img src="/img/b62a7058-7f87-4ca9-9a94-c0191eec31f3.png" width="400" />
<br/><br/>
