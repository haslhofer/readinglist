## Mixtral 8x7B with transformers, AWQ, and fused modules for improved accuracy and performance
Summary: A new method called Mistral 8x7B with fused modules and AWQ is introduced, powered by AutoAWQ and Transformers. Fused modules offer improved accuracy and performance. To use it, set a fuse_max_seq_len and set do_fuse=True in the quantization config during model initialization, and install Transformers from main.

Link: https://www.linkedin.com/posts/vaibhavs10_3x-faster-mixtral-8x7b-with-transformers-activity-7153781020985622528-_3qw?utm_source=share&amp;utm_medium=member_android

<img src="/img/1551300f-429b-4239-9cdb-b928833075d1.png" width="400" />
<br/><br/>
