## LLM Inference on Limited Memory Devices Using Flash Memory
Summary: Large language models (LLMs), crucial for natural language processing tasks, have intensive computational and memory requirements, making their deployment challenging on devices with limited DRAM capacity. To address this, a novel method is proposed, efficiently running LLMs that exceed the available DRAM by storing model parameters on flash memory and selectively loading them into DRAM. This method involves an inference cost model, optimized to minimize data transfer from flash and maximize data chunk sizes. Two key techniques are introduced: "windowing" reuses previously activated neurons, and "row-column bundling" exploits flash memory's sequential data access strengths. These techniques enable running models twice the DRAM size, with significant speed improvements compared to naive loading approaches. The integration of sparsity awareness, context-adaptive loading, and a hardware-oriented design paves the way for effective LLM inference on limited memory devices.

Link: https://arxiv.org/abs/2312.11514v1

<img src="/img/de143ecf-a2e1-463e-af33-3de37919f073.png" width="400" />
<br/><br/>
