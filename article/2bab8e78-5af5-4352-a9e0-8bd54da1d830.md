## Efficient Large Language Model Inference with Limited Memory
Summary: The provided text introduces a novel approach for efficient inference of Large Language Models (LLMs) on devices with limited DRAM capacity. The method involves storing model parameters on flash memory and bringing them on demand to DRAM, guided by an inference cost model that optimizes data transfer and utilizes the strengths of flash memory. Two techniques, "windowing" and "row-column bundling," are introduced to reduce data transfer and increase data chunk sizes. Integration of sparsity awareness, context-adaptive loading, and a hardware-oriented design enables effective inference of LLMs on devices with restricted memory, resulting in significant speed improvements.

Link: https://arxiv.org/abs/2312.11514

<img src="/img/2bab8e78-5af5-4352-a9e0-8bd54da1d830.png" width="400" />
<br/><br/>
