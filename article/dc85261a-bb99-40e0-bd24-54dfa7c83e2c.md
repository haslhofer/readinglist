## Efficient In-Memory Inference of Large Language Models with Limited DRAM Capacity
Summary: Researchers have developed a method to efficiently run large language models (LLMs) that exceed the available DRAM capacity by storing the model parameters on flash memory and bringing them on demand to DRAM. The proposed approach utilizes two techniques: windowing to reduce data transfer by reusing previously activated neurons, and row-column bundling to increase the size of data chunks read from flash memory. This allows for running models up to twice the size of the available DRAM, with improved inference speed compared to traditional loading approaches. The method integrates sparsity awareness, context-adaptive loading, and a hardware-oriented design, paving the way for effective inference of LLMs on devices with limited memory.

Link: https://arxiv.org/abs/2312.11514v1

<img src="/img/dc85261a-bb99-40e0-bd24-54dfa7c83e2c.png" width="400" />
<br/><br/>
