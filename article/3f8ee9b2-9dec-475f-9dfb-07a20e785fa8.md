## Multivariate Probabilistic Time Series Forecasting Using The Informer Model
Summary: In this post, the Informer model, a multivariate probabilistic time series forecasting model, is introduced. It is based on the vanilla Transformer model and employs two major improvements to make it more efficient for long sequence time-series forecasting. The first improvement is the use of ProbSparse attention, which reduces the computational complexity from 
𝑂
(
𝑇
2
𝐷
)
O(T
2
D) to 
𝑂
(
𝑇
log
⁡
𝑇
)
O(TlogT), where 
𝑇
T is the time series length and 
𝐷
D is the dimension of the hidden states. The second improvement is the use of a Distilling operation, which reduces the input size between encoder layers into its half slice, thus reducing the whole memory usage from 
𝑂
(
𝑁
𝑇
2
)
O(NT
2
) to 
𝑂
(
𝑁
⋅
𝑇
log
⁡
𝑇
)
O(N⋅TlogT), where 
𝑁
N is the number of encoder/decoder layers.

The Informer model is evaluated on the traffic_hourly dataset from the Monash Time Series Forecasting repository, which contains 862 hourly time series showing the road occupancy rates in the San Francisco Bay area freeways from 2015 to 2016. The model achieves a MASE (mean absolute scaled error) of 1.191 and a sMAPE (symmetric mean absolute percentage error) of 0.532 on the test set, which is competitive with other state-of-the-art models.

The Informer model is available in the 🤗 Transformers library and can be used for multivariate probabilistic time series forecasting tasks.

Link: https://huggingface.co/blog/informer

<img src="/img/3f8ee9b2-9dec-475f-9dfb-07a20e785fa8.png" width="400" />
<br/><br/>
