## Hugging Face: Bitsandbytes and Auto-GPTQ Quantization Schemes Compared
Summary: Hugging Face provides natively supported quantization schemes for PyTorch-based transformers models, allowing for inference on smaller devices and efficient fine-tuning of adapters. Two main methods, bitsandbytes and auto-gptq, are compared in terms of speed, performance degradation, and ease of use. Bitsandbytes offers zero-shot quantization and cross-modality interoperability, while auto-gptq is faster for text generation and supports n-bit quantization. The best approach depends on the specific use case, with a suggestion to use bitsandbytes for fine-tuning and GPTQ for deployment.

Link: https://huggingface.co/blog/overview-quantization-transformers

<img src="/img/f00fc40d-8de2-4991-be2d-440bb16d1893.png" width="400" />
<br/><br/>
